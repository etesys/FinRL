{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*.\n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fix Finrl error: python setup.py bdist_wheel did not run successfully\n",
        "!pip install Cmake\n",
        "!pip install swig\n",
        "#!pip3 install wheel\n",
        "#!conda install gymnasium[box2d]\n",
        "#FinRL error llmx 0.0.15a0 requires cohere, which is not installed.\n",
        "use_8bit_adam = False"
      ],
      "metadata": {
        "id": "0fNCMOcl-PYw",
        "outputId": "fc71cdf3-769c-4885-b59d-4ddb81f59347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Cmake in /usr/local/lib/python3.10/dist-packages (3.27.9)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.1.1.post1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D0vEcPxSJ8hI",
        "outputId": "75592d2f-3328-4508-c2b9-8a858c89f3f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-3pxduk4e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-3pxduk4e\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit e58b160332627c81886c50b3f58e69e1bbc7eb89\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-k1680s46/elegantrl_2d6bdbc211754749976696c157825a8b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-k1680s46/elegantrl_2d6bdbc211754749976696c157825a8b\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit b4b9d662b9f9cb7cc368ac2b1036b5119eb20be4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: alpaca-trade-api<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: ccxt<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.1.60)\n",
            "Requirement already satisfied: exchange-calendars<5,>=4 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (4.5)\n",
            "Requirement already satisfied: jqdatasdk<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.9.2)\n",
            "Requirement already satisfied: pyfolio<0.10,>=0.9 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.9.2)\n",
            "Requirement already satisfied: pyportfolioopt<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.5.5)\n",
            "Requirement already satisfied: ray[default,tune]<3,>=2 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (2.9.0)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.2.2)\n",
            "Requirement already satisfied: stable-baselines3[extra]>=2.0.0a5 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (2.2.1)\n",
            "Requirement already satisfied: stockstats<0.6,>=0.5 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.5.4)\n",
            "Requirement already satisfied: wrds<4,>=3 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (3.1.6)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.2.33)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.18)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.7.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.0.3)\n",
            "Requirement already satisfied: aiohttp==3.8.2 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.8.2)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.1)\n",
            "Requirement already satisfied: multidict<6.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (5.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (67.7.2)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (2023.11.17)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (41.0.7)\n",
            "Requirement already satisfied: aiodns>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.8.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2023.3)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.4.50)\n",
            "Requirement already satisfied: thriftpy2>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.4.17)\n",
            "Requirement already satisfied: pymysql>=0.7.6 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.0)\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.7.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.2)\n",
            "Requirement already satisfied: empyrical>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.19.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.20.3)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.5)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.14)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.1.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.3)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.10.13)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.19.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.4.0)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.21.0)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.60.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (10.0.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2023.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.2.0)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.29.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.15.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.7.0)\n",
            "Requirement already satisfied: shimmy[atari]~=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.10/dist-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.9)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.4.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.10)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.25.2)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.4.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.12)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.4.post1)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.10/dist-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (12.535.133)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.6)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.5.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.1)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.10/dist-packages (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (3.11.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.0.8)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (4.1.1.post1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.15.2)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.11.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.2.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.4.1->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from stable_baselines3.common.logger import configure\n",
        "\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process:\n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "id": "sLOL2le1BaCT",
        "outputId": "d788ec62-a61c-43ac-f03a-29416d093793",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(\"drive/My Drive/train_data.csv\")\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure\n",
        "# it has the columns and index in the form that could be make into the environment.\n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "80d039a8-c3c3-4f4c-fbd2-9649e35f4766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "45b79c86-b7d7-4f35-84f4-b4800527dfce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "7d21ac67-91ae-4cc5-9b86-4010c8dc9103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cpu device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "453f2f57-4abb-4ff1-c8ed-67d2abb6342f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 64         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 7          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0.0249     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -82.6      |\n",
            "|    reward             | 0.40442675 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 4.18       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 70         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 14         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.131      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -109       |\n",
            "|    reward             | -0.8270466 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 7.75       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 20       |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -487     |\n",
            "|    reward             | 4.480054 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 149      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 71        |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 28        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0.0706    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -2.92     |\n",
            "|    reward             | 1.5140209 |\n",
            "|    std                | 0.999     |\n",
            "|    value_loss         | 4.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 73         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 33         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.00934    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 674        |\n",
            "|    reward             | -15.875606 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 457        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 71          |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 42          |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0.00415     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 200         |\n",
            "|    reward             | 0.039996356 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 25          |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 73         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 47         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -135       |\n",
            "|    reward             | -3.7283964 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 12.2       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 71         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 55         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -0.01      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 32.9       |\n",
            "|    reward             | -2.1844127 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 2.46       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 61         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 150        |\n",
            "|    reward             | -1.1713659 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 17.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 71         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 69         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -11.5      |\n",
            "|    reward             | -2.8763871 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 0.971      |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 75        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0.000326  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -505      |\n",
            "|    reward             | 3.5862327 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 165       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 82         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -40.9      |\n",
            "|    explained_variance | -0.064     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -139       |\n",
            "|    reward             | 0.07029057 |\n",
            "|    std                | 0.993      |\n",
            "|    value_loss         | 13.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 89        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -40.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -127      |\n",
            "|    reward             | -5.681092 |\n",
            "|    std                | 0.993     |\n",
            "|    value_loss         | 16.2      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 73        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 95        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 152       |\n",
            "|    reward             | 0.6884773 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 22.4      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 103      |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41      |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -5.91    |\n",
            "|    reward             | 0.16727  |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 7.46     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 109       |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | 115       |\n",
            "|    reward             | 1.8186227 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 8.49      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 117       |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -365      |\n",
            "|    reward             | 3.3509936 |\n",
            "|    std                | 0.995     |\n",
            "|    value_loss         | 83.5      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 123        |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -50.8      |\n",
            "|    reward             | 0.93126166 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 1.8        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 131        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -9.42      |\n",
            "|    reward             | -0.4524674 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 0.677      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 137        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -3.58      |\n",
            "|    reward             | 0.38391006 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.26       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 2100       |\n",
            "|    time_elapsed       | 144        |\n",
            "|    total_timesteps    | 10500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2099       |\n",
            "|    policy_loss        | 209        |\n",
            "|    reward             | 0.15979892 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 47.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 151       |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | -456      |\n",
            "|    reward             | -8.174057 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 311       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 157        |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | -5.02e+03  |\n",
            "|    reward             | -5.5482683 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.71e+04   |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 165       |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.00173   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 112       |\n",
            "|    reward             | 0.1319164 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 13.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 2500      |\n",
            "|    time_elapsed       | 171       |\n",
            "|    total_timesteps    | 12500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2499      |\n",
            "|    policy_loss        | -95.5     |\n",
            "|    reward             | 0.1387407 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 5.36      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 2600      |\n",
            "|    time_elapsed       | 179       |\n",
            "|    total_timesteps    | 13000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2599      |\n",
            "|    policy_loss        | -13.1     |\n",
            "|    reward             | 0.6195741 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 0.759     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 2700       |\n",
            "|    time_elapsed       | 185        |\n",
            "|    total_timesteps    | 13500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2699       |\n",
            "|    policy_loss        | -34.2      |\n",
            "|    reward             | -2.8028793 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.06       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 193      |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 554      |\n",
            "|    reward             | 4.994293 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 239      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 199       |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -152      |\n",
            "|    reward             | 2.9080343 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 12.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3000      |\n",
            "|    time_elapsed       | 206       |\n",
            "|    total_timesteps    | 15000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2999      |\n",
            "|    policy_loss        | 114       |\n",
            "|    reward             | 1.6093862 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 7.62      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 3100       |\n",
            "|    time_elapsed       | 213        |\n",
            "|    total_timesteps    | 15500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3099       |\n",
            "|    policy_loss        | 46.9       |\n",
            "|    reward             | -0.4687722 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 2.04       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 219        |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | 210        |\n",
            "|    reward             | -1.5484804 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 61.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 227        |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 405        |\n",
            "|    reward             | -2.4117377 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 117        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 232       |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 301       |\n",
            "|    reward             | 25.834715 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 109       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 241       |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.01     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 182       |\n",
            "|    reward             | 0.8043928 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 25.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 246       |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0.00457   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -140      |\n",
            "|    reward             | 2.3185086 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 19.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 255       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 143       |\n",
            "|    reward             | 4.5387936 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 15        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 260       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | -0.0049   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 80.5      |\n",
            "|    reward             | 1.7981137 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.58      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 268       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -238      |\n",
            "|    reward             | 3.3932576 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 40.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 274       |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | 92        |\n",
            "|    reward             | 3.5655887 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 25.2      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 72          |\n",
            "|    iterations         | 4100        |\n",
            "|    time_elapsed       | 281         |\n",
            "|    total_timesteps    | 20500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4099        |\n",
            "|    policy_loss        | -33.5       |\n",
            "|    reward             | 0.057364248 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 3.27        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 4200       |\n",
            "|    time_elapsed       | 288        |\n",
            "|    total_timesteps    | 21000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4199       |\n",
            "|    policy_loss        | -277       |\n",
            "|    reward             | -1.0388125 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 47         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 4300      |\n",
            "|    time_elapsed       | 294       |\n",
            "|    total_timesteps    | 21500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4299      |\n",
            "|    policy_loss        | -125      |\n",
            "|    reward             | 4.9578304 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 15.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 302       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 32.2      |\n",
            "|    reward             | 2.3502576 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 7.69      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 4500     |\n",
            "|    time_elapsed       | 308      |\n",
            "|    total_timesteps    | 22500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4499     |\n",
            "|    policy_loss        | 665      |\n",
            "|    reward             | 4.440798 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 266      |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 4600       |\n",
            "|    time_elapsed       | 316        |\n",
            "|    total_timesteps    | 23000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4599       |\n",
            "|    policy_loss        | 187        |\n",
            "|    reward             | 0.23196213 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 31.8       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 4700       |\n",
            "|    time_elapsed       | 322        |\n",
            "|    total_timesteps    | 23500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4699       |\n",
            "|    policy_loss        | -101       |\n",
            "|    reward             | 0.23743176 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 13.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 330        |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -215       |\n",
            "|    reward             | -1.6252178 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 38         |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 4900      |\n",
            "|    time_elapsed       | 336       |\n",
            "|    total_timesteps    | 24500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4899      |\n",
            "|    policy_loss        | -147      |\n",
            "|    reward             | 1.3057066 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 18.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 5000     |\n",
            "|    time_elapsed       | 343      |\n",
            "|    total_timesteps    | 25000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4999     |\n",
            "|    policy_loss        | 108      |\n",
            "|    reward             | -1.1218  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 14.2     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 5100     |\n",
            "|    time_elapsed       | 350      |\n",
            "|    total_timesteps    | 25500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5099     |\n",
            "|    policy_loss        | 414      |\n",
            "|    reward             | 2.124065 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 120      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 357      |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | -397     |\n",
            "|    reward             | 7.42643  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 134      |\n",
            "------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3921931.38\n",
            "total_reward: 2921931.38\n",
            "total_cost: 7622.75\n",
            "total_trades: 47209\n",
            "Sharpe: 0.662\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 365        |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | -0.0026    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -47.1      |\n",
            "|    reward             | 0.31109712 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.33       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 371        |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | -169       |\n",
            "|    reward             | 0.50102633 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 19.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 379       |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 191       |\n",
            "|    reward             | 2.6564991 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 24.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 5600       |\n",
            "|    time_elapsed       | 385        |\n",
            "|    total_timesteps    | 28000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5599       |\n",
            "|    policy_loss        | -185       |\n",
            "|    reward             | 0.89243585 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 21.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 393        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | -256       |\n",
            "|    reward             | -1.0301471 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 49.2       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 72       |\n",
            "|    iterations         | 5800     |\n",
            "|    time_elapsed       | 399      |\n",
            "|    total_timesteps    | 29000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5799     |\n",
            "|    policy_loss        | -17.2    |\n",
            "|    reward             | 2.24742  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.49     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 5900       |\n",
            "|    time_elapsed       | 407        |\n",
            "|    total_timesteps    | 29500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5899       |\n",
            "|    policy_loss        | 49.5       |\n",
            "|    reward             | -0.5914899 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 1.76       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 413        |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0.024      |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | 68.8       |\n",
            "|    reward             | -0.2450351 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.04       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 6100       |\n",
            "|    time_elapsed       | 421        |\n",
            "|    total_timesteps    | 30500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6099       |\n",
            "|    policy_loss        | -33.9      |\n",
            "|    reward             | -4.0602837 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 4.48       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 427        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 49.7       |\n",
            "|    reward             | -1.5075847 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.08       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 434       |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | -186      |\n",
            "|    reward             | 1.6187354 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 46.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 442       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 74.4      |\n",
            "|    reward             | 2.0220625 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 3.89      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 448        |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | 49.2       |\n",
            "|    reward             | -4.8522453 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 14         |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 72          |\n",
            "|    iterations         | 6600        |\n",
            "|    time_elapsed       | 456         |\n",
            "|    total_timesteps    | 33000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6599        |\n",
            "|    policy_loss        | -71.9       |\n",
            "|    reward             | -0.26122797 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 6.98        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 462       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | -848      |\n",
            "|    reward             | -7.825993 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 478       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 6800       |\n",
            "|    time_elapsed       | 470        |\n",
            "|    total_timesteps    | 34000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6799       |\n",
            "|    policy_loss        | -221       |\n",
            "|    reward             | -0.9052406 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 33.4       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 476       |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | -406      |\n",
            "|    reward             | 4.4248095 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 158       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 7000       |\n",
            "|    time_elapsed       | 484        |\n",
            "|    total_timesteps    | 35000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | -0.00127   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6999       |\n",
            "|    policy_loss        | 87.2       |\n",
            "|    reward             | 0.11448185 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 5.13       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 490       |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -0.000268 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | -5.98     |\n",
            "|    reward             | 0.991272  |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 0.482     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7200      |\n",
            "|    time_elapsed       | 498       |\n",
            "|    total_timesteps    | 36000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7199      |\n",
            "|    policy_loss        | -464      |\n",
            "|    reward             | -1.131195 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 127       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7300      |\n",
            "|    time_elapsed       | 504       |\n",
            "|    total_timesteps    | 36500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7299      |\n",
            "|    policy_loss        | 90.2      |\n",
            "|    reward             | 3.5275168 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 14.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 512        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | -20        |\n",
            "|    reward             | -11.829491 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 26         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 519        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | -36.2      |\n",
            "|    reward             | -11.501397 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 5.36       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 525       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | -151      |\n",
            "|    reward             | 1.7744535 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 14.1      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 533       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | -184      |\n",
            "|    reward             | 0.9716358 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 23.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7800      |\n",
            "|    time_elapsed       | 539       |\n",
            "|    total_timesteps    | 39000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7799      |\n",
            "|    policy_loss        | -47.7     |\n",
            "|    reward             | -0.799004 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 4.39      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 547       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 249       |\n",
            "|    reward             | 5.27097   |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 83        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 553        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -298       |\n",
            "|    reward             | -2.3523793 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 84.5       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 561       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | -192      |\n",
            "|    reward             | 18.685633 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 30.9      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 72          |\n",
            "|    iterations         | 8200        |\n",
            "|    time_elapsed       | 567         |\n",
            "|    total_timesteps    | 41000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | 0.000713    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8199        |\n",
            "|    policy_loss        | -23.4       |\n",
            "|    reward             | -0.04035631 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 0.704       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 575        |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | 69.3       |\n",
            "|    reward             | -1.2028651 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 4.53       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 8400      |\n",
            "|    time_elapsed       | 581       |\n",
            "|    total_timesteps    | 42000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8399      |\n",
            "|    policy_loss        | -25.6     |\n",
            "|    reward             | -1.848898 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.72      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 8500       |\n",
            "|    time_elapsed       | 589        |\n",
            "|    total_timesteps    | 42500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8499       |\n",
            "|    policy_loss        | -15.9      |\n",
            "|    reward             | 0.48545647 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.32       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 8600       |\n",
            "|    time_elapsed       | 596        |\n",
            "|    total_timesteps    | 43000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8599       |\n",
            "|    policy_loss        | 163        |\n",
            "|    reward             | -22.215313 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 46.6       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 603       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | -11.1     |\n",
            "|    reward             | 0.9218878 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.76      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 610       |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | -70.9     |\n",
            "|    reward             | 0.9081576 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 3.49      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 72          |\n",
            "|    iterations         | 8900        |\n",
            "|    time_elapsed       | 617         |\n",
            "|    total_timesteps    | 44500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.9       |\n",
            "|    explained_variance | -0.0291     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8899        |\n",
            "|    policy_loss        | 123         |\n",
            "|    reward             | 0.054797325 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 8.68        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 71          |\n",
            "|    iterations         | 9000        |\n",
            "|    time_elapsed       | 625         |\n",
            "|    total_timesteps    | 45000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.9       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8999        |\n",
            "|    policy_loss        | 105         |\n",
            "|    reward             | -0.23676272 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 13.3        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 72        |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 631       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 13.5      |\n",
            "|    reward             | 0.8221327 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.61      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 71        |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 639       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | -200      |\n",
            "|    reward             | 2.7481256 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 40.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 72         |\n",
            "|    iterations         | 9300       |\n",
            "|    time_elapsed       | 645        |\n",
            "|    total_timesteps    | 46500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9299       |\n",
            "|    policy_loss        | -98.2      |\n",
            "|    reward             | 0.65849817 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 8.69       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 71        |\n",
            "|    iterations         | 9400      |\n",
            "|    time_elapsed       | 653       |\n",
            "|    total_timesteps    | 47000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9399      |\n",
            "|    policy_loss        | 156       |\n",
            "|    reward             | 0.3761728 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 17        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 71        |\n",
            "|    iterations         | 9500      |\n",
            "|    time_elapsed       | 659       |\n",
            "|    total_timesteps    | 47500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0.0317    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9499      |\n",
            "|    policy_loss        | -61.3     |\n",
            "|    reward             | 1.2846537 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 4.04      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 71        |\n",
            "|    iterations         | 9600      |\n",
            "|    time_elapsed       | 668       |\n",
            "|    total_timesteps    | 48000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9599      |\n",
            "|    policy_loss        | -144      |\n",
            "|    reward             | 1.6232065 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 16.3      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 71        |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 674       |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | 41.6      |\n",
            "|    reward             | 1.3728774 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 4.56      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 71       |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 682      |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.1    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | -150     |\n",
            "|    reward             | 8.144716 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 86.4     |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 71           |\n",
            "|    iterations         | 9900         |\n",
            "|    time_elapsed       | 688          |\n",
            "|    total_timesteps    | 49500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.2        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9899         |\n",
            "|    policy_loss        | 10.1         |\n",
            "|    reward             | -0.086703405 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 0.341        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 71         |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 695        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | 4.45       |\n",
            "|    reward             | -0.6611983 |\n",
            "|    std                | 1.04       |\n",
            "|    value_loss         | 1.33       |\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M2YadjfnLwgt",
        "outputId": "b10561de-e049-4148-8633-faea0d9d3c2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tCDa78rqfO_a",
        "outputId": "bba0c4f3-1e27-4fb8-c78f-820144d30479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5226783.16\n",
            "total_reward: 4226783.16\n",
            "total_cost: 5494.63\n",
            "total_trades: 51235\n",
            "Sharpe: 0.936\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 26       |\n",
            "|    time_elapsed    | 443      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -62.9    |\n",
            "|    critic_loss     | 462      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8679     |\n",
            "|    reward          | 2.594033 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 23       |\n",
            "|    time_elapsed    | 979      |\n",
            "|    total_timesteps | 23144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -33.3    |\n",
            "|    critic_loss     | 8.62     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 20251    |\n",
            "|    reward          | 2.594033 |\n",
            "---------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5395151.36\n",
            "total_reward: 4395151.36\n",
            "total_cost: 1059.43\n",
            "total_trades: 40488\n",
            "Sharpe: 0.893\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1520     |\n",
            "|    total_timesteps | 34716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.7    |\n",
            "|    critic_loss     | 5.31     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31823    |\n",
            "|    reward          | 2.594033 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 2074     |\n",
            "|    total_timesteps | 46288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -21.3    |\n",
            "|    critic_loss     | 3.91     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43395    |\n",
            "|    reward          | 2.594033 |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ne6M2R-WvrUQ",
        "outputId": "4d0bed6a-bd5f-49f4-eb3c-939a99ab991a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "y5D5PFUhMzSV",
        "outputId": "8731f4d0-97de-44d8-f2d0-75302ced4ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cpu device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Gt8eIQKYM4G3",
        "outputId": "d17c0584-3dc5-4bdd-f978-0bcd064ec92d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    fps             | 74        |\n",
            "|    iterations      | 1         |\n",
            "|    time_elapsed    | 27        |\n",
            "|    total_timesteps | 2048      |\n",
            "| train/             |           |\n",
            "|    reward          | 0.4753644 |\n",
            "----------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 74          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 54          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014426243 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.00442    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 3.54        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0254     |\n",
            "|    reward               | 1.2391683   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 11          |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4026080.17\n",
            "total_reward: 3026080.17\n",
            "total_cost: 335939.02\n",
            "total_trades: 80824\n",
            "Sharpe: 0.771\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014301237 |\n",
            "|    clip_fraction        | 0.189       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.0103      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.8        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.018      |\n",
            "|    reward               | -1.1301305  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 50.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 111         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018544102 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00209     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 68.3        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    reward               | 2.7491603   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 151         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 140         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022463528 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.0262     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.77        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0247     |\n",
            "|    reward               | 2.5497153   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 21.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 169         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013900033 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | 0.00594     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 55.2        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    reward               | 2.427917    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 66.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 198         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014781958 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0119      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.1        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.017      |\n",
            "|    reward               | -2.1166153  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 79.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 225         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017059047 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0116      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.8        |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    reward               | 0.34336177  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 91.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 252         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018429307 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.00242    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 103         |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0212     |\n",
            "|    reward               | 0.29421467  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 178         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 10         |\n",
            "|    time_elapsed         | 282        |\n",
            "|    total_timesteps      | 20480      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02089666 |\n",
            "|    clip_fraction        | 0.224      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | 0.00753    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 25.5       |\n",
            "|    n_updates            | 90         |\n",
            "|    policy_gradient_loss | -0.0172    |\n",
            "|    reward               | 1.5697337  |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 69.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 310         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014471427 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.0051     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 46.8        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0218     |\n",
            "|    reward               | -1.8141268  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 142         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 339         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014187567 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.0486      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 31.3        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    reward               | 0.26006624  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 79.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 367         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013130545 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.0117     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 218         |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0174     |\n",
            "|    reward               | 0.4021925   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 500         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 394         |\n",
            "|    total_timesteps      | 28672       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018358361 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.000488   |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 143         |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    reward               | 0.23772943  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 157         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 421        |\n",
            "|    total_timesteps      | 30720      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02019307 |\n",
            "|    clip_fraction        | 0.214      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.6      |\n",
            "|    explained_variance   | -0.0152    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 12.9       |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -0.0197    |\n",
            "|    reward               | 4.2259445  |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 39.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 450         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034258194 |\n",
            "|    clip_fraction        | 0.299       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.0247      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37.6        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    reward               | 0.29386804  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 54          |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4850023.76\n",
            "total_reward: 3850023.76\n",
            "total_cost: 316028.39\n",
            "total_trades: 78667\n",
            "Sharpe: 0.824\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 478         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024161816 |\n",
            "|    clip_fraction        | 0.282       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.00771    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.4        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0236     |\n",
            "|    reward               | 1.0406977   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 52.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 506         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033420578 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.00813     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.6        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | -0.3654697  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 109         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 534         |\n",
            "|    total_timesteps      | 38912       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026648723 |\n",
            "|    clip_fraction        | 0.277       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.0177      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.3        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.0178     |\n",
            "|    reward               | -0.538564   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 21.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 561         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022643771 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.8       |\n",
            "|    explained_variance   | 0.00837     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 53          |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    reward               | 0.2061389   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 82.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 589         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026355563 |\n",
            "|    clip_fraction        | 0.302       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.00382    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 48.9        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    reward               | -9.530682   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 97.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 617         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024725622 |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | -0.0222     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.9        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    reward               | 2.49447     |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 39.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 646         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022759946 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.0265      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 69.8        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    reward               | -1.6768086  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 73.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 674         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028193962 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.0612      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27          |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    reward               | 4.3256726   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 62.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 702         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025823012 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | -0.0462     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 44.9        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0173     |\n",
            "|    reward               | -0.42992377 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 83.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 728         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017575469 |\n",
            "|    clip_fraction        | 0.182       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0795      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10          |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0161     |\n",
            "|    reward               | 1.2750986   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 32.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 757         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020329352 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0178      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.8        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    reward               | -2.030511   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 52.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 786         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016673103 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.0129      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.5        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0218     |\n",
            "|    reward               | -0.25232872 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 66.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 29         |\n",
            "|    time_elapsed         | 814        |\n",
            "|    total_timesteps      | 59392      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02279668 |\n",
            "|    clip_fraction        | 0.217      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.3      |\n",
            "|    explained_variance   | 0.0274     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 20.3       |\n",
            "|    n_updates            | 280        |\n",
            "|    policy_gradient_loss | -0.014     |\n",
            "|    reward               | -1.1472464 |\n",
            "|    std                  | 1.04       |\n",
            "|    value_loss           | 31.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 843         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019639682 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.0324      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.4        |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0148     |\n",
            "|    reward               | 0.9074779   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 56.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 869         |\n",
            "|    total_timesteps      | 63488       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02240679  |\n",
            "|    clip_fraction        | 0.227       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | -0.0098     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.5        |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    reward               | -0.81017774 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 73.2        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3733199.84\n",
            "total_reward: 2733199.84\n",
            "total_cost: 292680.79\n",
            "total_trades: 75470\n",
            "Sharpe: 0.633\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 32         |\n",
            "|    time_elapsed         | 897        |\n",
            "|    total_timesteps      | 65536      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02255183 |\n",
            "|    clip_fraction        | 0.275      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.5      |\n",
            "|    explained_variance   | -0.0103    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 17.4       |\n",
            "|    n_updates            | 310        |\n",
            "|    policy_gradient_loss | -0.0134    |\n",
            "|    reward               | 0.609032   |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 46         |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 33         |\n",
            "|    time_elapsed         | 926        |\n",
            "|    total_timesteps      | 67584      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02811517 |\n",
            "|    clip_fraction        | 0.325      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.6      |\n",
            "|    explained_variance   | 0.0166     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 26.2       |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -0.019     |\n",
            "|    reward               | 0.30831578 |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 83.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 954         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033985876 |\n",
            "|    clip_fraction        | 0.313       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0162      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.2        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.00509    |\n",
            "|    reward               | 1.262502    |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 112         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 35         |\n",
            "|    time_elapsed         | 982        |\n",
            "|    total_timesteps      | 71680      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03036787 |\n",
            "|    clip_fraction        | 0.282      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.8      |\n",
            "|    explained_variance   | 0.00709    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 101        |\n",
            "|    n_updates            | 340        |\n",
            "|    policy_gradient_loss | -0.00435   |\n",
            "|    reward               | 0.74843514 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 145        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 1010        |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018379191 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.0747      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.9        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    reward               | -4.755577   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 23.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 1037        |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023319658 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.0179      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24          |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    reward               | -0.6237432  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 62.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 1065        |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029602176 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.00909     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 111         |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00916    |\n",
            "|    reward               | -17.119066  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 140         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 39         |\n",
            "|    time_elapsed         | 1094       |\n",
            "|    total_timesteps      | 79872      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02701894 |\n",
            "|    clip_fraction        | 0.295      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43        |\n",
            "|    explained_variance   | 0.0356     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 35.8       |\n",
            "|    n_updates            | 380        |\n",
            "|    policy_gradient_loss | -0.0117    |\n",
            "|    reward               | -2.5407157 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 79         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 1122        |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027182184 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.0341      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 97.5        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0168     |\n",
            "|    reward               | -0.45422333 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 120         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 1151        |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024095614 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0343      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 72.9        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0096     |\n",
            "|    reward               | 0.25711003  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 150         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 72         |\n",
            "|    iterations           | 42         |\n",
            "|    time_elapsed         | 1178       |\n",
            "|    total_timesteps      | 86016      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04220097 |\n",
            "|    clip_fraction        | 0.354      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.2      |\n",
            "|    explained_variance   | 0.0131     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 57         |\n",
            "|    n_updates            | 410        |\n",
            "|    policy_gradient_loss | -0.00842   |\n",
            "|    reward               | 0.41317695 |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 140        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 1205        |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025001798 |\n",
            "|    clip_fraction        | 0.234       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.231       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.2        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0204     |\n",
            "|    reward               | -1.8847266  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 26.3        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 72           |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 1234         |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.025397338  |\n",
            "|    clip_fraction        | 0.208        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -43.3        |\n",
            "|    explained_variance   | 0.166        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 47.4         |\n",
            "|    n_updates            | 430          |\n",
            "|    policy_gradient_loss | -0.00899     |\n",
            "|    reward               | -0.017755862 |\n",
            "|    std                  | 1.08         |\n",
            "|    value_loss           | 122          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 1262        |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030273138 |\n",
            "|    clip_fraction        | 0.278       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.0796      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.1        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00212    |\n",
            "|    reward               | -2.004308   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 69          |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3881623.78\n",
            "total_reward: 2881623.78\n",
            "total_cost: 291958.54\n",
            "total_trades: 74897\n",
            "Sharpe: 0.685\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 1291        |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039277412 |\n",
            "|    clip_fraction        | 0.294       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.143       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.4        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00587    |\n",
            "|    reward               | -4.941118   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 28.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 72          |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 1319        |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026168294 |\n",
            "|    clip_fraction        | 0.273       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.0907      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.3        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    reward               | 1.7365825   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 75.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 1345        |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019254198 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.0893      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.4        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.0114     |\n",
            "|    reward               | 10.319355   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 60.2        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 1373       |\n",
            "|    total_timesteps      | 100352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02340252 |\n",
            "|    clip_fraction        | 0.239      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.6      |\n",
            "|    explained_variance   | -0.0487    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 31.2       |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.0116    |\n",
            "|    reward               | -1.5297751 |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 61.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 1402        |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025791619 |\n",
            "|    clip_fraction        | 0.277       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.0615      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.21        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    reward               | 0.011341811 |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 19.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1430        |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020286117 |\n",
            "|    clip_fraction        | 0.207       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.0501      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 39.1        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.0147     |\n",
            "|    reward               | -0.60391384 |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 68.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1458        |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035684846 |\n",
            "|    clip_fraction        | 0.281       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.0444      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 68.8        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    reward               | -4.3985434  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 116         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 53         |\n",
            "|    time_elapsed         | 1485       |\n",
            "|    total_timesteps      | 108544     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03435459 |\n",
            "|    clip_fraction        | 0.28       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.8      |\n",
            "|    explained_variance   | 0.00765    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 24.1       |\n",
            "|    n_updates            | 520        |\n",
            "|    policy_gradient_loss | -0.0101    |\n",
            "|    reward               | 1.2555104  |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 45.9       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 54         |\n",
            "|    time_elapsed         | 1512       |\n",
            "|    total_timesteps      | 110592     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03745243 |\n",
            "|    clip_fraction        | 0.279      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.9      |\n",
            "|    explained_variance   | 0.0306     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 96.5       |\n",
            "|    n_updates            | 530        |\n",
            "|    policy_gradient_loss | -0.0166    |\n",
            "|    reward               | 0.88832414 |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 225        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 1541        |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024555253 |\n",
            "|    clip_fraction        | 0.219       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.168       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 92.1        |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00883    |\n",
            "|    reward               | 2.1782346   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 136         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1569        |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021822661 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.124       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.3        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | 1.5818176   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 28.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1597        |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.043319732 |\n",
            "|    clip_fraction        | 0.332       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.117       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 20.1        |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | -0.74404263 |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 54.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 1625        |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034705177 |\n",
            "|    clip_fraction        | 0.282       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.0736      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 90.8        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | 2.2099476   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 121         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 59         |\n",
            "|    time_elapsed         | 1652       |\n",
            "|    total_timesteps      | 120832     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02175455 |\n",
            "|    clip_fraction        | 0.299      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.1      |\n",
            "|    explained_variance   | 0.0112     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 64.1       |\n",
            "|    n_updates            | 580        |\n",
            "|    policy_gradient_loss | -0.0105    |\n",
            "|    reward               | 0.8781142  |\n",
            "|    std                  | 1.11       |\n",
            "|    value_loss           | 135        |\n",
            "----------------------------------------\n",
            "day: 2892, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3247953.75\n",
            "total_reward: 2247953.75\n",
            "total_cost: 298658.59\n",
            "total_trades: 75651\n",
            "Sharpe: 0.626\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 1680        |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030728437 |\n",
            "|    clip_fraction        | 0.269       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.287       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 6.26        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.00584    |\n",
            "|    reward               | -0.32908028 |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 19.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 1708        |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032061316 |\n",
            "|    clip_fraction        | 0.284       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.131       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 46.9        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    reward               | 1.1374786   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 62.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1736        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030856123 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.0528      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 66.7        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00799    |\n",
            "|    reward               | 8.912043    |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 91.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 1764        |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.045610778 |\n",
            "|    clip_fraction        | 0.359       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.0813      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.9        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00965    |\n",
            "|    reward               | 2.9162018   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 38.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 1790        |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028598316 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.0223      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 61.6        |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.0159     |\n",
            "|    reward               | -0.27619433 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 88.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1818        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025605876 |\n",
            "|    clip_fraction        | 0.256       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.0259      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.9        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    reward               | -1.5370169  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 46          |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 66         |\n",
            "|    time_elapsed         | 1847       |\n",
            "|    total_timesteps      | 135168     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04528238 |\n",
            "|    clip_fraction        | 0.353      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.4      |\n",
            "|    explained_variance   | -0.0635    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 21.6       |\n",
            "|    n_updates            | 650        |\n",
            "|    policy_gradient_loss | -0.00768   |\n",
            "|    reward               | 1.9119093  |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 63.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 67          |\n",
            "|    time_elapsed         | 1875        |\n",
            "|    total_timesteps      | 137216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02511735  |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.138       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.7        |\n",
            "|    n_updates            | 660         |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    reward               | -0.94893205 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 28          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1903        |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028101593 |\n",
            "|    clip_fraction        | 0.219       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.0181      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 199         |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.0136     |\n",
            "|    reward               | -0.33450723 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 186         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 1930        |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.041005515 |\n",
            "|    clip_fraction        | 0.367       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.0374      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 88          |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00143    |\n",
            "|    reward               | 5.4649553   |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 146         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 70         |\n",
            "|    time_elapsed         | 1957       |\n",
            "|    total_timesteps      | 143360     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02508093 |\n",
            "|    clip_fraction        | 0.218      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.6      |\n",
            "|    explained_variance   | 0.0504     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 11.7       |\n",
            "|    n_updates            | 690        |\n",
            "|    policy_gradient_loss | -0.0138    |\n",
            "|    reward               | 0.77289116 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 28.1       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 71         |\n",
            "|    time_elapsed         | 1985       |\n",
            "|    total_timesteps      | 145408     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03887208 |\n",
            "|    clip_fraction        | 0.323      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.0768     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 86.3       |\n",
            "|    n_updates            | 700        |\n",
            "|    policy_gradient_loss | -0.00708   |\n",
            "|    reward               | 0.3895101  |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 100        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 2013        |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031779516 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.0601      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 58          |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | -0.00927    |\n",
            "|    reward               | -22.695498  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 158         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 2042        |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031279325 |\n",
            "|    clip_fraction        | 0.351       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.066       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 58.3        |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.0029     |\n",
            "|    reward               | -3.1216733  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 88.2        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5109650.44\n",
            "total_reward: 4109650.44\n",
            "total_cost: 282018.89\n",
            "total_trades: 74922\n",
            "Sharpe: 0.769\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 2068        |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026110783 |\n",
            "|    clip_fraction        | 0.288       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.0264      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 49.2        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.00957    |\n",
            "|    reward               | -2.246056   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 80.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 2095        |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033426575 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.0883      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 130         |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | 2.6255603   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 151         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 2124        |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019916128 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.0026      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 268         |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00118    |\n",
            "|    reward               | -5.3091373  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 429         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 2152        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.046592735 |\n",
            "|    clip_fraction        | 0.297       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.158       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.2        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | -0.00238    |\n",
            "|    reward               | 1.2406349   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 38.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 78         |\n",
            "|    time_elapsed         | 2181       |\n",
            "|    total_timesteps      | 159744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02658816 |\n",
            "|    clip_fraction        | 0.23       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.9      |\n",
            "|    explained_variance   | 0.0666     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 116        |\n",
            "|    n_updates            | 770        |\n",
            "|    policy_gradient_loss | -0.0119    |\n",
            "|    reward               | 2.0639024  |\n",
            "|    std                  | 1.14       |\n",
            "|    value_loss           | 189        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 79         |\n",
            "|    time_elapsed         | 2209       |\n",
            "|    total_timesteps      | 161792     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03007466 |\n",
            "|    clip_fraction        | 0.334      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.9      |\n",
            "|    explained_variance   | 0.00673    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 57.8       |\n",
            "|    n_updates            | 780        |\n",
            "|    policy_gradient_loss | 0.00467    |\n",
            "|    reward               | 9.506342   |\n",
            "|    std                  | 1.14       |\n",
            "|    value_loss           | 195        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 2236        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030420057 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0388      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37.3        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.009      |\n",
            "|    reward               | -0.03653542 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 98.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 2264        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.0328466   |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.00469     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 53.6        |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00544    |\n",
            "|    reward               | -0.42286572 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 188         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 2293        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024076223 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.021       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.1        |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.00986    |\n",
            "|    reward               | -0.968919   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 159         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 2321        |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027767897 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0762      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 166         |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | -0.00708    |\n",
            "|    reward               | 3.1857169   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 209         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 2350        |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029898152 |\n",
            "|    clip_fraction        | 0.301       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | -0.085      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12.9        |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | -0.00579    |\n",
            "|    reward               | 0.38434517  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 23.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 2379        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026033808 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.0766      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 176         |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.00167     |\n",
            "|    reward               | -0.2565424  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 229         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 2407        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039423473 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | -0.00897    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 120         |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | -0.00605    |\n",
            "|    reward               | -1.5043339  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 213         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 2434        |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.044233188 |\n",
            "|    clip_fraction        | 0.308       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.000968    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.7        |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.00625    |\n",
            "|    reward               | 1.3431113   |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 57.4        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4015581.34\n",
            "total_reward: 3015581.34\n",
            "total_cost: 295139.11\n",
            "total_trades: 75900\n",
            "Sharpe: 0.692\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 2462        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028360795 |\n",
            "|    clip_fraction        | 0.244       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.00948     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.3        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | -3.1216452  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 185         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 73           |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 2491         |\n",
            "|    total_timesteps      | 182272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.033972725  |\n",
            "|    clip_fraction        | 0.276        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -45.2        |\n",
            "|    explained_variance   | 0.0266       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 40.4         |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.0117      |\n",
            "|    reward               | -0.062199973 |\n",
            "|    std                  | 1.15         |\n",
            "|    value_loss           | 80.3         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 90         |\n",
            "|    time_elapsed         | 2519       |\n",
            "|    total_timesteps      | 184320     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03047701 |\n",
            "|    clip_fraction        | 0.252      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.3      |\n",
            "|    explained_variance   | 0.0442     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 71.6       |\n",
            "|    n_updates            | 890        |\n",
            "|    policy_gradient_loss | -0.0122    |\n",
            "|    reward               | 0.22656882 |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 198        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 73         |\n",
            "|    iterations           | 91         |\n",
            "|    time_elapsed         | 2547       |\n",
            "|    total_timesteps      | 186368     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03557576 |\n",
            "|    clip_fraction        | 0.36       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.3      |\n",
            "|    explained_variance   | 0.179      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 9.23       |\n",
            "|    n_updates            | 900        |\n",
            "|    policy_gradient_loss | 0.0035     |\n",
            "|    reward               | -0.566226  |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 21.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 2574        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030998323 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.14        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.3        |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -0.0105     |\n",
            "|    reward               | -1.3274597  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 2601        |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048201814 |\n",
            "|    clip_fraction        | 0.353       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.0748      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 75.5        |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | 0.00633     |\n",
            "|    reward               | -2.1058042  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 79.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 2630        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022551274 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.098       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 20.3        |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    reward               | 0.15883487  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 43.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 2658        |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025746426 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.4       |\n",
            "|    explained_variance   | 0.0153      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 138         |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.00892    |\n",
            "|    reward               | -1.0701299  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 137         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 2687        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032833043 |\n",
            "|    clip_fraction        | 0.29        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.4       |\n",
            "|    explained_variance   | 0.0657      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 98.2        |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.00695    |\n",
            "|    reward               | -1.8917674  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 161         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 2715        |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027371053 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.4       |\n",
            "|    explained_variance   | 0.102       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.9        |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    reward               | 2.2867892   |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 62.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 73          |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 2741        |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026072208 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.5       |\n",
            "|    explained_variance   | 0.086       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28          |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    reward               | 0.36942276  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 107         |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "C6AidlWyvwzm",
        "outputId": "d8a39b9b-d99a-46fa-9a68-ebe3a98322bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JSAHhV4Xc-bh",
        "outputId": "38ad10d1-8941-49f3-830c-191f6f8832be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cpu device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OSRxNYAxdKpU",
        "outputId": "fd7f36e0-beac-4547-8c04-56ff13e1a3ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 110\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5859706.84\n",
            "total_reward: 4859706.84\n",
            "total_cost: 1515.92\n",
            "total_trades: 37778\n",
            "Sharpe: 0.829\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 4           |\n",
            "|    fps             | 27          |\n",
            "|    time_elapsed    | 415         |\n",
            "|    total_timesteps | 11572       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 203         |\n",
            "|    critic_loss     | 3.72e+03    |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 8679        |\n",
            "|    reward          | -0.68082285 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 8           |\n",
            "|    fps             | 24          |\n",
            "|    time_elapsed    | 940         |\n",
            "|    total_timesteps | 23144       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 113         |\n",
            "|    critic_loss     | 725         |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 20251       |\n",
            "|    reward          | -0.68082285 |\n",
            "------------------------------------\n",
            "day: 2892, episode: 120\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5859706.84\n",
            "total_reward: 4859706.84\n",
            "total_cost: 1515.92\n",
            "total_trades: 37778\n",
            "Sharpe: 0.829\n",
            "=================================\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 12          |\n",
            "|    fps             | 23          |\n",
            "|    time_elapsed    | 1470        |\n",
            "|    total_timesteps | 34716       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 84.6        |\n",
            "|    critic_loss     | 134         |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 31823       |\n",
            "|    reward          | -0.68082285 |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    episodes        | 16          |\n",
            "|    fps             | 23          |\n",
            "|    time_elapsed    | 2008        |\n",
            "|    total_timesteps | 46288       |\n",
            "| train/             |             |\n",
            "|    actor_loss      | 71.1        |\n",
            "|    critic_loss     | 26.1        |\n",
            "|    learning_rate   | 0.001       |\n",
            "|    n_updates       | 43395       |\n",
            "|    reward          | -0.68082285 |\n",
            "------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OkJV6V_mv2hw",
        "outputId": "202935d5-1e36-4622-a312-2ca4292d3d4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xwOhVjqRkCdM",
        "outputId": "d6a34616-1e5d-420f-a631-b17c92d34e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cpu device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "K8RSdKCckJyH",
        "outputId": "9e9ef914-a42b-41d4-cb4f-28d75df2b6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day: 2892, episode: 130\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5666902.74\n",
            "total_reward: 4666902.74\n",
            "total_cost: 229329.98\n",
            "total_trades: 59660\n",
            "Sharpe: 0.918\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 21       |\n",
            "|    time_elapsed    | 537      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.19e+03 |\n",
            "|    critic_loss     | 152      |\n",
            "|    ent_coef        | 0.243    |\n",
            "|    ent_coef_loss   | -36.5    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 11471    |\n",
            "|    reward          | 4.584844 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 21        |\n",
            "|    time_elapsed    | 1091      |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 477       |\n",
            "|    critic_loss     | 30.5      |\n",
            "|    ent_coef        | 0.0794    |\n",
            "|    ent_coef_loss   | -110      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 3.4639661 |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 20       |\n",
            "|    time_elapsed    | 1659     |\n",
            "|    total_timesteps | 34716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 239      |\n",
            "|    critic_loss     | 24.8     |\n",
            "|    ent_coef        | 0.0253   |\n",
            "|    ent_coef_loss   | -140     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 34615    |\n",
            "|    reward          | 11.96807 |\n",
            "---------------------------------\n",
            "day: 2892, episode: 140\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 8020365.47\n",
            "total_reward: 7020365.47\n",
            "total_cost: 9167.28\n",
            "total_trades: 51918\n",
            "Sharpe: 1.019\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 20        |\n",
            "|    time_elapsed    | 2220      |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 133       |\n",
            "|    critic_loss     | 41.2      |\n",
            "|    ent_coef        | 0.00817   |\n",
            "|    ent_coef_loss   | -138      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 11.979915 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 20        |\n",
            "|    time_elapsed    | 2782      |\n",
            "|    total_timesteps | 57860     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 74.1      |\n",
            "|    critic_loss     | 11.2      |\n",
            "|    ent_coef        | 0.00273   |\n",
            "|    ent_coef_loss   | -83.7     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 57759     |\n",
            "|    reward          | 11.294062 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 150\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6944477.89\n",
            "total_reward: 5944477.89\n",
            "total_cost: 2089.15\n",
            "total_trades: 44006\n",
            "Sharpe: 1.012\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 20       |\n",
            "|    time_elapsed    | 3348     |\n",
            "|    total_timesteps | 69432    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 44.9     |\n",
            "|    critic_loss     | 10.7     |\n",
            "|    ent_coef        | 0.00138  |\n",
            "|    ent_coef_loss   | 1.28     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 69331    |\n",
            "|    reward          | 9.286095 |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_SpZoQgPv7GO",
        "outputId": "edf24fb7-3bdf-4e31-ee8e-5b6689f1cf0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp trained_models/*.zip \"drive/My Drive/\""
      ],
      "metadata": {
        "id": "6LOfhPz-YGR3"
      },
      "execution_count": 43,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1ofncK2cYhs"
      },
      "source": [
        "Disclaimer: Nothing herein is financial advice, and NOT a recommendation to trade real money. Many platforms exist for simulated trading (paper trading) which can be used for building and developing the methods discussed. Please use common sense and always first consult a professional before trading or investing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZpyg1Anea4G"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/etesys/FinRL/blob/master/3-Practical/FinRL_PaperTrading_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3mbRu3s1YlD"
      },
      "source": [
        "# Part 1: Install FinRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gkmsPgbvNf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8cd3bc-556d-4a71-d007-ad8aa0eb52f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wrds\n",
            "  Downloading wrds-3.2.0-py3-none-any.whl (13 kB)\n",
            "Collecting numpy<1.27,>=1.26 (from wrds)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<23.3 (from wrds)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<2.3,>=2.2 (from wrds)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psycopg2-binary<2.10,>=2.9 (from wrds)\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.13,>=1.12 (from wrds)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy<2.1,>=2 in /usr/local/lib/python3.10/dist-packages (from wrds) (2.0.28)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->wrds) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->wrds) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas<2.3,>=2.2->wrds)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.1,>=2->wrds) (4.10.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "Installing collected packages: tzdata, psycopg2-binary, packaging, numpy, scipy, pandas, wrds\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.4\n",
            "    Uninstalling scipy-1.11.4:\n",
            "      Successfully uninstalled scipy-1.11.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.26.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 packaging-23.2 pandas-2.2.1 psycopg2-binary-2.9.9 scipy-1.12.0 tzdata-2024.1 wrds-3.2.0\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "⏬ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:13\n",
            "🔁 Restarting kernel...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package swig4.0.\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting git+https://github.com/etesys/FinRL.git\n",
            "  Cloning https://github.com/etesys/FinRL.git to /tmp/pip-req-build-nq8ivpn5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/etesys/FinRL.git /tmp/pip-req-build-nq8ivpn5\n",
            "  Resolved https://github.com/etesys/FinRL.git to commit eb74d05159bd50b82ae2c677c954c72909d0fa9c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-khdhw00n/elegantrl_f4bbe8d0c45b432f84de8fcd3634a080\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-khdhw00n/elegantrl_f4bbe8d0c45b432f84de8fcd3634a080\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 87cf325af4c0608f1e74941091a3bc277dfaf739\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange-calendars<5,>=4 (from finrl==0.3.6)\n",
            "  Downloading exchange_calendars-4.5.3-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting jqdatasdk<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading jqdatasdk-1.9.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting pyfolio<0.10,>=0.9 (from finrl==0.3.6)\n",
            "  Downloading pyfolio-0.9.2.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyportfolioopt<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading pyportfolioopt-1.5.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ray<3,>=2 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting scikit-learn<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting stable-baselines3>=2.0.0a5 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading stable_baselines3-2.3.0a4-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting stockstats<0.6,>=0.5 (from finrl==0.3.6)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl.metadata (26 kB)\n",
            "Collecting wrds<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading wrds-3.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting yfinance<0.3,>=0.2 (from finrl==0.3.6)\n",
            "  Downloading yfinance-0.2.37-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pandas>=0.18.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting numpy>=1.11.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client<2,>=0.56.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading msgpack-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Collecting aiohttp<4,>=3.8.3 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (68.2.2)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (2023.11.17)\n",
            "Collecting cryptography>=2.6.1 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiodns-3.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting yarl>=1.7.2 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting pyluach (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting toolz (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tzdata (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting korean-lunar-calendar (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting six (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting SQLAlchemy>=1.2.8 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting thriftpy2>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading thriftpy2-0.4.20.tar.gz (689 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m689.0/689.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading PyMySQL-1.1.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting ipython>=3.2.3 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading ipython-8.22.2-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting matplotlib>=1.4.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting pytz>=2014.10 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting scipy>=0.14.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seaborn>=0.7.1 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting empyrical>=0.5.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cvxpy<2.0.0,>=1.1.19 (from pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading cvxpy-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting click>=7.0 (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting filelock (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting jsonschema (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting protobuf!=3.19.5,>=3.15.3 (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading protobuf-5.26.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting aiosignal (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting frozenlist (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Collecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client>=0.7.1 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading prometheus_client-0.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting smart-open (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading virtualenv-20.25.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting grpcio>=1.42.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pyarrow>=6.0.1 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting fsspec (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn<2,>=1->finrl==0.3.6)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting threadpoolctl>=2.0.0 (from scikit-learn<2,>=1->finrl==0.3.6)\n",
            "  Downloading threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting torch>=1.13 (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting cloudpickle (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting opencv-python (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pygame (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting psutil (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.1)\n",
            "Collecting rich (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pillow (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds<4,>=3->finrl==0.3.6)\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting multitasking>=0.0.7 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lxml>=4.9.1 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting appdirs>=1.4.4 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting frozendict>=2.3.4 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading frozendict-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading peewee-3.17.1.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting beautifulsoup4>=4.11.1 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting html5lib>=1.1 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting gym (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6)\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading pycares-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.16.0)\n",
            "Collecting osqp>=0.6.2 (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading osqp-0.6.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting ecos>=2 (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading ecos-2.0.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Collecting clarabel>=0.5.0 (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading clarabel-0.7.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting scs>=3.0 (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading scs-3.2.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting pybind11 (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pandas-datareader>=0.2 (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pandas_datareader-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.3.0 (from gymnasium<0.30,>=0.28.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting webencodings (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting decorator (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting matplotlib-inline (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading prompt_toolkit-3.0.43-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pygments>=2.4.0 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting stack-data (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5.13.0 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading traitlets-5.14.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting exceptiongroup (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pexpect>4.3 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading contourpy-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading fonttools-4.50.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.16.3 (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.6)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting absl-py>=0.4 (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting ply<4.0,>=3.4 (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Collecting sympy (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting jinja2 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (4.1.0)\n",
            "Collecting gym-notices>=0.0.4 (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6)\n",
            "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d]->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6)\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting swig==4.* (from gym[box2d]->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6)\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading referencing-0.34.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading rpds_py-0.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting wrapt (from smart-open->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting importlib-resources (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
            "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf!=3.19.5,>=3.15.3 (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting parso<0.9.0,>=0.8.3 (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "INFO: pip is looking at multiple versions of osqp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting osqp>=0.6.2 (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading osqp-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting qdldl (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading qdldl-0.1.7.post0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting mpmath>=0.19 (from sympy->torch>=1.13->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading msgpack-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.7/323.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.5.3-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.7/191.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jqdatasdk-1.9.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyportfolioopt-1.5.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.3.0a4-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Downloading wrds-3.2.0-py3-none-any.whl (13 kB)\n",
            "Downloading yfinance-0.2.37-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiodns-3.1.1-py3-none-any.whl (5.4 kB)\n",
            "Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Downloading cryptography-42.0.5-cp39-abi3-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cvxpy-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozendict-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.22.2-py3-none-any.whl (811 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.0/812.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_client-0.20.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m629.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:07:44\u001b[0m"
          ]
        }
      ],
      "source": [
        "#import pdb\n",
        "## install finrl library\n",
        "!pip install wrds\n",
        "!pip install swig\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "!pip install git+https://github.com/etesys/FinRL.git  ## HSG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--6Kx8I21erH"
      },
      "source": [
        "## Import related modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install zipline-reloaded  ## HSG for zipline not available error\n",
        "#!pip install zipline\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.config import INDICATORS\n",
        "from finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv # HSG\n",
        "#!git clone https://github.com/etesys/FinRL/ my_finrl\n",
        "#from my_finrl.finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv  # HSG mod to use local modifiable copy of env_stocktrading_np.py\n",
        "from finrl.meta.env_stock_trading.env_stock_papertrading import AlpacaPaperTrading\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "QrDwT9LpNJkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip show numpy\n",
        "#!pip install --upgrade numpy\n",
        "#from my_finrl import tree as env_stock_trading_np\n",
        "#cd my_finrl\n",
        "#pip install https://colab.research.google.com/github/etesys/FinRL/blob/master/3-Practical/FinRL_PaperTrading_Demo.ipynb\n",
        "#!pip install https://colab.research.google.com/github/etesys/FinRL/blob/master/finrl/meta/env_stock_trading/env_stocktrading_np.py\n",
        "#import env_stocktrading_np as StockTradingEnv\n",
        "\n",
        "#!git clone https://github.com/etesys/FinRL/tree/master/finrl/meta/env_stock_trading/ my_finrl2\n",
        "\n",
        "#!git clone https://github.com/etesys/FinRL/ my_finrl\n",
        "#!pip install my_finrl.finrl.meta.env_stock_trading.env_stocktrading_np\n",
        "\n",
        "# this one works: from my_finrl.finrl.meta.env_stock_trading.env_stocktrading_np import StockTradingEnv\n",
        "\n",
        "#import StockTradingEnv\n",
        "\n",
        "#!git clone https://github.com/etesys/FinRL my_finrl\n",
        "#!pip install my_env/tree/master/finrl/\n",
        "#from my_env import env_stock_trading_np\n",
        "#!git clone https://github.com/etesys/FinRL/tree/master/finrl/meta my_env #/tree/master/finrl/meta/env_stock_trading\n",
        "#from my_env/tree/master/finrl/meta/env_stock_trading import env_stock_trading_np\n",
        "#as StockTradingEnv\n",
        "#from https://github.com/etesys/FinRL/tree/master/finrl/meta/env_stock_trading/ import env_stock_trading_np #.py as StockTradingEnv\n",
        "#from env_stock_trading import env_stock_trading_np as StockTradingEnv\n"
      ],
      "metadata": {
        "id": "42fm6InQW-tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EVJIQUR6_fu"
      },
      "source": [
        "## PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of stock trading, states, actions, and rewards are used to define the reinforcement learning problem. States are representations of the market at a given time, actions are the trades that can be made, and rewards are the profits or losses that are realized from those trades.\n",
        "\n",
        "States are typically represented as vectors of features that describe the market. These features can include things like the prices of different stocks, the volume of trading, and the news sentiment. Actions are typically represented as integers that correspond to different trade types, such as buying, selling, or holding. Rewards are typically represented as real numbers that indicate the profit or loss from a trade.\n",
        "\n",
        "The goal of reinforcement learning in stock trading is to learn a policy that maps states to actions in order to maximize the expected reward. This policy can then be used to trade stocks on behalf of a trader.\n",
        "\n",
        "The calculation of states, actions, and rewards is a complex process that involves a number of different factors. However, the basic principles are as follows:\n",
        "\n",
        "States are typically calculated by taking a snapshot of the market at a given time. This snapshot can include information such as the prices of different stocks, the volume of trading, and the news sentiment.\n",
        "Actions are typically represented as integers that correspond to different trade types, such as buying, selling, or holding.\n",
        "Rewards are typically calculated by taking the profit or loss from a trade. This profit or loss can be calculated by subtracting the purchase price of a stock from the sale price.\n",
        "The reinforcement learning problem in stock trading is a challenging one, but it is also a very exciting one. By learning to trade stocks using reinforcement learning, traders can potentially achieve significant returns."
      ],
      "metadata": {
        "id": "u7Ai7YMpz700"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the context of stock trading, an actor-critic model is a type of reinforcement learning agent that uses two neural networks to learn how to trade stocks. The actor network takes the current state of the market as input and outputs a trade action, such as buying or selling a stock. The critic network takes the current state of the market and the trade action as input and outputs a value function, which is an estimate of the expected return from taking that action.\n",
        "\n",
        "The actor network is trained to maximize the value function output by the critic network. This is done by playing a game of self-play, in which the actor network trades stocks and the critic network evaluates the performance of those trades. Over time, the actor network learns to take actions that maximize the value function, and the critic network learns to provide accurate estimates of the value function.\n",
        "\n",
        "Actor-critic models have been shown to be very effective for stock trading. They can learn to trade stocks in a way that outperforms human traders. This is because actor-critic models can learn to take into account a wide range of factors when making trading decisions, including the current state of the market, the historical performance of stocks, and the news sentiment.\n",
        "\n",
        "Here is a more detailed explanation of the actor-critic model in the context of stock trading:\n",
        "\n",
        "Actor network: The actor network is a neural network that takes the current state of the market as input and outputs a trade action, such as buying or selling a stock. The actor network is trained to maximize the value function output by the critic network.\n",
        "Critic network: The critic network is a neural network that takes the current state of the market and the trade action as input and outputs a value function, which is an estimate of the expected return from taking that action. The critic network is trained to provide accurate estimates of the value function.\n",
        "Self-play: The actor-critic model is trained by playing a game of self-play. In self-play, the actor network trades stocks and the critic network evaluates the performance of those trades. Over time, the actor network learns to take actions that maximize the value function, and the critic network learns to provide accurate estimates of the value function.\n",
        "Actor-critic models have been shown to be very effective for stock trading. They can learn to trade stocks in a way that outperforms human traders. This is because actor-critic models can learn to take into account a wide range of factors when making trading decisions, including the current state of the market, the historical performance of stocks, and the news sentiment."
      ],
      "metadata": {
        "id": "nCSg_NYd2KTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reinforcement learning, rewards are used to guide the agent towards desirable states and actions. In the context of stock trading, rewards can be calculated based on a variety of factors, such as the profit or loss from a trade, the change in the stock price, or the volume of trading.\n",
        "\n",
        "One common way to calculate rewards in stock trading is to use a fixed reward. This means that the reward for a trade is always the same, regardless of the profit or loss. For example, you could use a reward of 1 for a successful trade and a reward of -1 for an unsuccessful trade.\n",
        "\n",
        "Another way to calculate rewards is to use a proportional reward. This means that the reward for a trade is proportional to the profit or loss. For example, you could use a reward of 0.1 for a profit of $100 and a reward of -0.1 for a loss of $$100.\n",
        "\n",
        "Finally, you can also use a dynamic reward. This means that the reward for a trade is calculated based on a variety of factors, such as the current state of the market, the historical performance of the stock, and the news sentiment.\n",
        "\n",
        "The best way to calculate rewards in stock trading depends on your specific trading strategy. You should experiment with different reward functions to find one that works well for you.\n",
        "\n",
        "Here are some additional tips for calculating rewards in stock trading:\n",
        "\n",
        "- Make sure that the rewards are consistent. This means that the same reward should be given for the same action, regardless of the context.\n",
        "\n",
        "- Make sure that the rewards are meaningful. The rewards should be something that the agent can understand and learn from.\n",
        "\n",
        "- Make sure that the rewards are sparse. This means that the rewards should only be given for important events, such as successful trades.\n",
        "\n",
        "By following these tips, you can create a reward function that will help your reinforcement learning agent learn to trade stocks effectively."
      ],
      "metadata": {
        "id": "5cz4dt5H9Ga7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reward dynamically affects the subsequent states and actions in a reinforcement learning problem by influencing the agent's policy. The policy is a function that maps states to actions, and it is learned by the agent through interaction with the environment. The reward signal tells the agent how well it is doing, and it is used to update the policy so that the agent takes actions that are more likely to lead to high rewards.\n",
        "\n",
        "In the context of stock trading, the reward could be based on the profit or loss from a trade, or on some other measure of success. The reward signal would then be used to update the policy so that the agent takes actions that are more likely to lead to profitable trades.\n",
        "\n",
        "For example, if the agent takes an action that results in a high reward, then the policy will be updated to make it more likely that the agent will take that action in the future. Similarly, if the agent takes an action that results in a low reward, then the policy will be updated to make it less likely that the agent will take that action in the future.\n",
        "\n",
        "Over time, the agent will learn to take actions that are more likely to lead to high rewards, and its policy will become more optimal.\n",
        "\n",
        "Here is a more detailed explanation of how the reward dynamically affects the subsequent states and actions:\n",
        "\n",
        "The reward signal tells the agent how well it is doing. When the agent takes an action that results in a high reward, it means that the agent is doing something right. This encourages the agent to continue taking actions that are similar to the one that resulted in the high reward.\n",
        "The reward signal helps the agent to learn. The agent learns by interacting with the environment and receiving feedback in the form of rewards. The rewards help the agent to identify the actions that are more likely to lead to high rewards, and it uses this information to update its policy.\n",
        "The reward signal guides the agent towards optimal behavior. Over time, the agent will learn to take actions that are more likely to lead to high rewards. This means that the agent will become more optimal in its behavior.\n",
        "The reward signal is a powerful tool that can be used to guide the agent towards optimal behavior. By carefully designing the reward signal, you can help the agent to learn to solve the problem that you are trying to solve.\n",
        "\n"
      ],
      "metadata": {
        "id": "cc9Ib3Kz6pBh"
      }
    },
    {
      "source": [
        "\n",
        "'''\n",
        "def policy_function(state):\n",
        "  # Get the current market conditions.\n",
        "  prices = state['prices']\n",
        "  volume = state['volume']\n",
        "  news_sentiment = state['news_sentiment']\n",
        "\n",
        "  # Calculate the trader's desired actions.\n",
        "  actions = []\n",
        "  for stock in prices:\n",
        "    if news_sentiment[stock] == 'positive':\n",
        "      actions.append('buy')\n",
        "    elif news_sentiment[stock] == 'negative':\n",
        "      actions.append('sell')\n",
        "    else:\n",
        "      if volume[stock] > volume[stock] - 1:\n",
        "        actions.append('buy')\n",
        "      else:\n",
        "        actions.append('sell')\n",
        "\n",
        "  # Return the trader's desired actions.\n",
        "  return actions\n",
        "  '''\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BcmnJ4sBEtUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "super().__init__() is a Python method that calls the constructor of the parent class. In your code, the parent class is nn.Module, which is the base class for all neural network models in PyTorch. Calling super().__init__() ensures that the parent class's constructor is called, which initializes the model's parameters and sets up any necessary state.\n"
      ],
      "metadata": {
        "id": "JUL0UxDz4-bK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def forward(self, x) is the forward pass of the neural network model. This is the method that is called when you pass data through the model to make predictions. In your code, the forward pass takes the input data x and passes it through the model's layers, returning the output predictions."
      ],
      "metadata": {
        "id": "WELxSf8k5XAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EYx40S84tzo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "import numpy.random as rd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from torch import Tensor\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "\n",
        "class ActorPPO(nn.Module):\n",
        "    def __init__(self, dims: [int], state_dim: int, action_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = build_mlp(dims=[state_dim, *dims, action_dim])\n",
        "        self.action_std_log = nn.Parameter(torch.zeros((1, action_dim)), requires_grad=True)  # trainable parameter\n",
        "\n",
        "    def forward(self, state: Tensor) -> Tensor:\n",
        "        return self.net(state).tanh()  # action.tanh()\n",
        "\n",
        "    def get_action(self, state: Tensor) -> (Tensor, Tensor):  # for exploration\n",
        "        action_avg = self.net(state)\n",
        "        action_std = self.action_std_log.exp()\n",
        "\n",
        "        dist = Normal(action_avg, action_std)\n",
        "        action = dist.sample()\n",
        "        logprob = dist.log_prob(action).sum(1) # HSG\n",
        "        #logprob = dist.log_prob(action)\n",
        "        return action, logprob\n",
        "\n",
        "    def get_logprob_entropy(self, state: Tensor, action: Tensor) -> (Tensor, Tensor):\n",
        "        action_avg = self.net(state)\n",
        "        action_std = self.action_std_log.exp()\n",
        "\n",
        "        dist = Normal(action_avg, action_std)\n",
        "        logprob = dist.log_prob(action).sum(1)\n",
        "        entropy = dist.entropy().sum(1)\n",
        "        return logprob, entropy\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_action_for_env(action: Tensor) -> Tensor:\n",
        "        return action.tanh()\n",
        "\n",
        "\n",
        "class CriticPPO(nn.Module):\n",
        "    def __init__(self, dims: [int], state_dim: int, _action_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = build_mlp(dims=[state_dim, *dims, 1])\n",
        "\n",
        "    def forward(self, state: Tensor) -> Tensor:\n",
        "        return self.net(state)  # advantage value\n",
        "\n",
        "\n",
        "def build_mlp(dims: [int]) -> nn.Sequential:  # MLP (MultiLayer Perceptron)\n",
        "    net_list = []\n",
        "    for i in range(len(dims) - 1):\n",
        "        net_list.extend([nn.Linear(dims[i], dims[i + 1]), nn.ReLU()])\n",
        "    del net_list[-1]  # remove the activation of output layer\n",
        "    return nn.Sequential(*net_list)\n",
        "'''\n",
        "# HSG - LSTM version to replace build_mlp\n",
        "def build_lstm(dims: [int]) -> nn.Sequential:  # LSTM (Long Short-Term Memory)\n",
        "    net_list = []\n",
        "    for i in range(len(dims) - 1):\n",
        "        net_list.extend([nn.LSTM(dims[i], dims[i + 1]), nn.ReLU()])\n",
        "    del net_list[-1]  # remove the activation of output layer\n",
        "    net_list.append(nn.Linear(dims[-1], dims[-1]))\n",
        "    return nn.Sequential(*net_list)\n",
        "'''\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, agent_class=None, env_class=None, env_args=None):\n",
        "        self.env_class = env_class  # env = env_class(**env_args)\n",
        "        self.env_args = env_args  # env = env_class(**env_args)\n",
        "\n",
        "        if env_args is None:  # dummy env_args\n",
        "            env_args = {'env_name': None, 'state_dim': None, 'action_dim': None, 'if_discrete': None}\n",
        "        self.env_name = env_args['env_name']  # the name of environment. Be used to set 'cwd'.\n",
        "        self.state_dim = env_args['state_dim']  # vector dimension (feature number) of state\n",
        "        self.action_dim = env_args['action_dim']  # vector dimension (feature number) of action\n",
        "        self.if_discrete = env_args['if_discrete']  # discrete or continuous action space\n",
        "\n",
        "        self.agent_class = agent_class  # agent = agent_class(...)\n",
        "\n",
        "        '''Arguments for reward shaping'''\n",
        "        self.gamma = 0.99  # discount factor of future rewards\n",
        "        self.reward_scale = 1.0  # an approximate target reward usually be closed to 256\n",
        "\n",
        "        '''Arguments for training'''\n",
        "        self.gpu_id = int(0)  # `int` means the ID of single GPU, -1 means CPU\n",
        "        self.net_dims = (64, 32)  # the middle layer dimension of MLP (MultiLayer Perceptron)\n",
        "        self.learning_rate = 6e-5  # 2 ** -14 ~= 6e-5\n",
        "        self.soft_update_tau = 5e-3  # 2 ** -8 ~= 5e-3\n",
        "        self.batch_size = int(128)  # num of transitions sampled from replay buffer.\n",
        "        self.horizon_len = int(2000)  # collect horizon_len step while exploring, then update network\n",
        "        self.buffer_size = None  # ReplayBuffer size. Empty the ReplayBuffer for on-policy.\n",
        "        self.repeat_times = 8.0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
        "\n",
        "        '''Arguments for evaluate'''\n",
        "        self.cwd = None  # current working directory to save model. None means set automatically\n",
        "        self.break_step = +np.inf  # break training if 'total_step > break_step'\n",
        "        self.eval_times = int(32)  # number of times that get episodic cumulative return\n",
        "        self.eval_per_step = int(2e4)  # evaluate the agent per training steps\n",
        "\n",
        "    def init_before_training(self):\n",
        "        if self.cwd is None:  # set cwd (current working directory) for saving model\n",
        "            self.cwd = f'./{self.env_name}_{self.agent_class.__name__[5:]}'\n",
        "        os.makedirs(self.cwd, exist_ok=True)\n",
        "\n",
        "\n",
        "def get_gym_env_args(env, if_print: bool) -> dict:\n",
        "    if {'unwrapped', 'observation_space', 'action_space', 'spec'}.issubset(dir(env)):  # isinstance(env, gym.Env):\n",
        "        env_name = env.unwrapped.spec.id\n",
        "        state_shape = env.observation_space.shape\n",
        "        state_dim = state_shape[0] if len(state_shape) == 1 else state_shape  # sometimes state_dim is a list\n",
        "\n",
        "        if_discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
        "        if if_discrete:  # make sure it is discrete action space\n",
        "            action_dim = env.action_space.n\n",
        "        elif isinstance(env.action_space, gym.spaces.Box):  # make sure it is continuous action space\n",
        "            action_dim = env.action_space.shape[0]\n",
        "\n",
        "    env_args = {'env_name': env_name, 'state_dim': state_dim, 'action_dim': action_dim, 'if_discrete': if_discrete}\n",
        "    print(f\"env_args = {repr(env_args)}\") if if_print else None\n",
        "    return env_args\n",
        "\n",
        "\n",
        "def kwargs_filter(function, kwargs: dict) -> dict:\n",
        "    import inspect\n",
        "    sign = inspect.signature(function).parameters.values()\n",
        "    sign = {val.name for val in sign}\n",
        "    common_args = sign.intersection(kwargs.keys())\n",
        "    return {key: kwargs[key] for key in common_args}  # filtered kwargs\n",
        "\n",
        "\n",
        "def build_env(env_class=None, env_args=None):\n",
        "    if env_class.__module__ == 'gym.envs.registration':  # special rule\n",
        "        env = env_class(id=env_args['env_name'])\n",
        "    else:\n",
        "        env = env_class(**kwargs_filter(env_class.__init__, env_args.copy()))\n",
        "    for attr_str in ('env_name', 'state_dim', 'action_dim', 'if_discrete'):\n",
        "        setattr(env, attr_str, env_args[attr_str])\n",
        "    return env\n",
        "\n",
        "\n",
        "class AgentBase:\n",
        "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.gamma = args.gamma\n",
        "        self.batch_size = args.batch_size\n",
        "        self.repeat_times = args.repeat_times\n",
        "        self.reward_scale = args.reward_scale\n",
        "        self.soft_update_tau = args.soft_update_tau\n",
        "\n",
        "        self.states = None  # assert self.states == (1, state_dim)\n",
        "        self.device = torch.device(f\"cuda:{gpu_id}\" if (torch.cuda.is_available() and (gpu_id >= 0)) else \"cpu\")\n",
        "\n",
        "        act_class = getattr(self, \"act_class\", None)\n",
        "        cri_class = getattr(self, \"cri_class\", None)\n",
        "        self.act = self.act_target = act_class(net_dims, state_dim, action_dim).to(self.device)\n",
        "        self.cri = self.cri_target = cri_class(net_dims, state_dim, action_dim).to(self.device) \\\n",
        "            if cri_class else self.act\n",
        "\n",
        "        self.act_optimizer = torch.optim.Adam(self.act.parameters(), args.learning_rate)\n",
        "        self.cri_optimizer = torch.optim.Adam(self.cri.parameters(), args.learning_rate) \\\n",
        "            if cri_class else self.act_optimizer\n",
        "\n",
        "        self.criterion = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    @staticmethod\n",
        "    def optimizer_update(optimizer, objective: Tensor):\n",
        "        optimizer.zero_grad()\n",
        "        objective.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_update(target_net: torch.nn.Module, current_net: torch.nn.Module, tau: float):\n",
        "        for tar, cur in zip(target_net.parameters(), current_net.parameters()):\n",
        "            tar.data.copy_(cur.data * tau + tar.data * (1.0 - tau))\n",
        "\n",
        "\n",
        "class AgentPPO(AgentBase):\n",
        "    def __init__(self, net_dims: [int], state_dim: int, action_dim: int, gpu_id: int = 0, args: Config = Config()):\n",
        "        self.if_off_policy = False\n",
        "        self.act_class = getattr(self, \"act_class\", ActorPPO)\n",
        "        self.cri_class = getattr(self, \"cri_class\", CriticPPO)\n",
        "        AgentBase.__init__(self, net_dims, state_dim, action_dim, gpu_id, args)\n",
        "\n",
        "        self.ratio_clip = getattr(args, \"ratio_clip\", 0.25)  # `ratio.clamp(1 - clip, 1 + clip)`\n",
        "        self.lambda_gae_adv = getattr(args, \"lambda_gae_adv\", 0.95)  # could be 0.80~0.99\n",
        "        self.lambda_entropy = getattr(args, \"lambda_entropy\", 0.01)  # could be 0.00~0.10\n",
        "        self.lambda_entropy = torch.tensor(self.lambda_entropy, dtype=torch.float32, device=self.device)\n",
        "\n",
        "    def explore_env(self, env, horizon_len: int) -> [Tensor]:\n",
        "        states = torch.zeros((horizon_len, self.state_dim), dtype=torch.float32).to(self.device)\n",
        "        actions = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
        "        logprobs = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)  # HSG 14/3/2024\n",
        "        #logprobs = torch.zeros((horizon_len, self.action_dim), dtype=torch.float32).to(self.device)\n",
        "        rewards = torch.zeros(horizon_len, dtype=torch.float32).to(self.device)\n",
        "        dones = torch.zeros(horizon_len, dtype=torch.bool).to(self.device)\n",
        "\n",
        "        ary_state = self.states[0]\n",
        "\n",
        "        get_action = self.act.get_action\n",
        "        convert = self.act.convert_action_for_env\n",
        "        #pdb.set_trace()\n",
        "        for i in range(horizon_len):\n",
        "            #if i==1948:\n",
        "            #  pdb.set_trace()\n",
        "            state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device) # HSG added unsqueeze\n",
        "            #state = torch.as_tensor(ary_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            action, logprob = [t.squeeze(0) for t in get_action(state.unsqueeze(0))[:2]]\n",
        "\n",
        "            ary_action = convert(action).detach().cpu().numpy()\n",
        "            #ary_state, reward, done, _ = env.step(ary_action)  # HSG\n",
        "            ary_state, reward, done, _, _ = env.step(ary_action)\n",
        "            if done:\n",
        "                ary_state = env.reset()\n",
        "\n",
        "\n",
        "            states[i] = state\n",
        "            actions[i] = action\n",
        "            logprobs[i] = logprob\n",
        "            rewards[i] = reward\n",
        "            dones[i] = done\n",
        "\n",
        "        self.states[0] = ary_state\n",
        "        rewards = (rewards * self.reward_scale).unsqueeze(1)\n",
        "        undones = (1 - dones.type(torch.float32)).unsqueeze(1)\n",
        "        return states, actions, logprobs, rewards, undones\n",
        "\n",
        "    def update_net(self, buffer) -> [float]:\n",
        "        with torch.no_grad():\n",
        "            states, actions, logprobs, rewards, undones = buffer\n",
        "            buffer_size = states.shape[0]\n",
        "\n",
        "            '''get advantages reward_sums'''\n",
        "            bs = 2 ** 10  # set a smaller 'batch_size' when out of GPU memory.\n",
        "            values = [self.cri(states[i:i + bs]) for i in range(0, buffer_size, bs)]\n",
        "            values = torch.cat(values, dim=0).squeeze(1)  # values.shape == (buffer_size, )\n",
        "\n",
        "            advantages = self.get_advantages(rewards, undones, values)  # advantages.shape == (buffer_size, )\n",
        "            reward_sums = advantages + values  # reward_sums.shape == (buffer_size, )\n",
        "            del rewards, undones, values\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std(dim=0) + 1e-5)\n",
        "            #pdb.set_trace()\n",
        "        #assert logprobs.shape == advantages.shape == reward_sums.shape == (buffer_size,)  # HSG 15/3/2024\n",
        "        assert (logprobs.size()[0],) == advantages.shape == reward_sums.shape == (buffer_size,)\n",
        "\n",
        "        '''update network'''\n",
        "        obj_critics = 0.0\n",
        "        obj_actors = 0.0\n",
        "\n",
        "        update_times = int(buffer_size * self.repeat_times / self.batch_size)\n",
        "        assert update_times >= 1\n",
        "        for _ in range(update_times):\n",
        "            indices = torch.randint(buffer_size, size=(self.batch_size,), requires_grad=False)\n",
        "            state = states[indices]\n",
        "            action = actions[indices]\n",
        "            logprob = logprobs[indices]\n",
        "            advantage = advantages[indices]\n",
        "            reward_sum = reward_sums[indices]\n",
        "\n",
        "            value = self.cri(state).squeeze(1)  # critic network predicts the reward_sum (Q value) of state\n",
        "            obj_critic = self.criterion(value, reward_sum)\n",
        "            self.optimizer_update(self.cri_optimizer, obj_critic)\n",
        "\n",
        "            new_logprob, obj_entropy = self.act.get_logprob_entropy(state, action)\n",
        "            ##pdb.set_trace()\n",
        "            ratio = (new_logprob - logprob.detach()).exp()\n",
        "            surrogate1 = advantage * ratio\n",
        "            surrogate2 = advantage * ratio.clamp(1 - self.ratio_clip, 1 + self.ratio_clip)\n",
        "            obj_surrogate = torch.min(surrogate1, surrogate2).mean()\n",
        "\n",
        "            obj_actor = obj_surrogate + obj_entropy.mean() * self.lambda_entropy\n",
        "            self.optimizer_update(self.act_optimizer, -obj_actor)\n",
        "\n",
        "            obj_critics += obj_critic.item()\n",
        "            obj_actors += obj_actor.item()\n",
        "        a_std_log = getattr(self.act, 'a_std_log', torch.zeros(1)).mean()\n",
        "        return obj_critics / update_times, obj_actors / update_times, a_std_log.item()\n",
        "\n",
        "    def get_advantages(self, rewards: Tensor, undones: Tensor, values: Tensor) -> Tensor:\n",
        "        advantages = torch.empty_like(values)  # advantage value\n",
        "\n",
        "        masks = undones * self.gamma\n",
        "        horizon_len = rewards.shape[0]\n",
        "\n",
        "        next_state = torch.tensor(self.states, dtype=torch.float32).to(self.device)\n",
        "        next_value = self.cri(next_state).detach()[0, 0]\n",
        "\n",
        "        advantage = 0  # last_gae_lambda\n",
        "        for t in range(horizon_len - 1, -1, -1):\n",
        "            delta = rewards[t] + masks[t] * next_value - values[t]\n",
        "            advantages[t] = advantage = delta + masks[t] * self.lambda_gae_adv * advantage\n",
        "            next_value = values[t]\n",
        "        return advantages\n",
        "\n",
        "\n",
        "class PendulumEnv(gym.Wrapper):  # a demo of custom gym env\n",
        "    def __init__(self):\n",
        "        gym.logger.set_level(40)  # Block warning\n",
        "        gym_env_name = \"Pendulum-v0\" if gym.__version__ < '0.18.0' else \"Pendulum-v1\"\n",
        "        super().__init__(env=gym.make(gym_env_name))\n",
        "\n",
        "        '''the necessary env information when you design a custom env'''\n",
        "        self.env_name = gym_env_name  # the name of this env.\n",
        "        self.state_dim = self.observation_space.shape[0]  # feature number of state\n",
        "        self.action_dim = self.action_space.shape[0]  # feature number of action\n",
        "        self.if_discrete = False  # discrete action or continuous action\n",
        "\n",
        "    def reset(self) -> np.ndarray:  # reset the agent in env\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):  # agent interacts in env\n",
        "        # We suggest that adjust action space to (-1, +1) when designing a custom env.\n",
        "        #state, reward, done, info_dict = self.env.step(action * 2) # HSG\n",
        "        state, reward, done, _, info_dict = self.env.step(action * 2)\n",
        "        #done = done.all()  # HSG added line\n",
        "        return state.reshape(self.state_dim), float(reward), done, info_dict\n",
        "\n",
        "\n",
        "def train_agent(args: Config):\n",
        "    args.init_before_training()\n",
        "\n",
        "    env = build_env(args.env_class, args.env_args)\n",
        "    agent = args.agent_class(args.net_dims, args.state_dim, args.action_dim, gpu_id=args.gpu_id, args=args)\n",
        "    agent.states = env.reset()[np.newaxis, :]\n",
        "    #env_reset = env.reset()\n",
        "    #env_reset = env.reset()[:-1] # HSG - exclude last state in env.reset() as it is None.\n",
        "    #agent.states = env_reset[0][np.newaxis, :]\n",
        "\n",
        "    evaluator = Evaluator(eval_env=build_env(args.env_class, args.env_args),\n",
        "                          eval_per_step=args.eval_per_step,\n",
        "                          eval_times=args.eval_times,\n",
        "                          cwd=args.cwd)\n",
        "    torch.set_grad_enabled(False)\n",
        "    while True: # start training\n",
        "        buffer_items = agent.explore_env(env, args.horizon_len)\n",
        "\n",
        "        torch.set_grad_enabled(True)\n",
        "        logging_tuple = agent.update_net(buffer_items)\n",
        "        torch.set_grad_enabled(False)\n",
        "\n",
        "        evaluator.evaluate_and_save(agent.act, args.horizon_len, logging_tuple)\n",
        "        if (evaluator.total_step > args.break_step) or os.path.exists(f\"{args.cwd}/stop\"):\n",
        "            torch.save(agent.act.state_dict(), args.cwd + '/actor.pth')\n",
        "            break  # stop training when reach `break_step` or `mkdir cwd/stop`\n",
        "\n",
        "\n",
        "def render_agent(env_class, env_args: dict, net_dims: [int], agent_class, actor_path: str, render_times: int = 8):\n",
        "    env = build_env(env_class, env_args)\n",
        "\n",
        "    state_dim = env_args['state_dim']\n",
        "    action_dim = env_args['action_dim']\n",
        "    agent = agent_class(net_dims, state_dim, action_dim, gpu_id=-1)\n",
        "    actor = agent.act\n",
        "\n",
        "    print(f\"| render and load actor from: {actor_path}\")\n",
        "    actor.load_state_dict(torch.load(actor_path, map_location=lambda storage, loc: storage))\n",
        "    for i in range(render_times):\n",
        "        cumulative_reward, episode_step = get_rewards_and_steps(env, actor, if_render=True)\n",
        "        print(f\"|{i:4}  cumulative_reward {cumulative_reward:9.3f}  episode_step {episode_step:5.0f}\")\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, eval_env, eval_per_step: int = 1e4, eval_times: int = 8, cwd: str = '.'):\n",
        "        self.cwd = cwd\n",
        "        self.env_eval = eval_env\n",
        "        self.eval_step = 0\n",
        "        self.total_step = 0\n",
        "        self.start_time = time.time()\n",
        "        self.eval_times = eval_times  # number of times that get episodic cumulative return\n",
        "        self.eval_per_step = eval_per_step  # evaluate the agent per training steps\n",
        "\n",
        "        self.recorder = []\n",
        "        print(f\"\\n| `step`: Number of samples, or total training steps, or running times of `env.step()`.\"\n",
        "              f\"\\n| `time`: Time spent from the start of training to this moment.\"\n",
        "              f\"\\n| `avgR`: Average value of cumulative rewards, which is the sum of rewards in an episode.\"\n",
        "              f\"\\n| `stdR`: Standard dev of cumulative rewards, which is the sum of rewards in an episode.\"\n",
        "              f\"\\n| `avgS`: Average of steps in an episode.\"\n",
        "              f\"\\n| `objC`: Objective of Critic network. Or call it loss function of critic network.\"\n",
        "              f\"\\n| `objA`: Objective of Actor network. It is the average Q value of the critic network.\"\n",
        "              f\"\\n| {'step':>8}  {'time':>8}  | {'avgR':>8}  {'stdR':>6}  {'avgS':>6}  | {'objC':>8}  {'objA':>8}\")\n",
        "\n",
        "    def evaluate_and_save(self, actor, horizon_len: int, logging_tuple: tuple):\n",
        "        self.total_step += horizon_len\n",
        "        if self.eval_step + self.eval_per_step > self.total_step:\n",
        "            return\n",
        "        self.eval_step = self.total_step\n",
        "\n",
        "        rewards_steps_ary = [get_rewards_and_steps(self.env_eval, actor) for _ in range(self.eval_times)]\n",
        "        rewards_steps_ary = np.array(rewards_steps_ary, dtype=np.float32)\n",
        "        avg_r = rewards_steps_ary[:, 0].mean()  # average of cumulative rewards\n",
        "        std_r = rewards_steps_ary[:, 0].std()  # std of cumulative rewards\n",
        "        avg_s = rewards_steps_ary[:, 1].mean()  # average of steps in an episode\n",
        "\n",
        "        used_time = time.time() - self.start_time\n",
        "        self.recorder.append((self.total_step, used_time, avg_r))\n",
        "\n",
        "        print(f\"| {self.total_step:8.2e}  {used_time:8.0f}  \"\n",
        "              f\"| {avg_r:8.2f}  {std_r:6.2f}  {avg_s:6.0f}  \"\n",
        "              f\"| {logging_tuple[0]:8.2f}  {logging_tuple[1]:8.2f}\")\n",
        "\n",
        "\n",
        "def get_rewards_and_steps(env, actor, if_render: bool = False) -> (float, int):  # cumulative_rewards and episode_steps\n",
        "    device = next(actor.parameters()).device  # net.parameters() is a Python generator.\n",
        "\n",
        "    state = env.reset()\n",
        "    episode_steps = 0\n",
        "    cumulative_returns = 0.0  # sum of rewards in an episode\n",
        "    for episode_steps in range(12345):\n",
        "        tensor_state = torch.as_tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        tensor_action = actor(tensor_state)\n",
        "        action = tensor_action.detach().cpu().numpy()[0]  # not need detach(), because using torch.no_grad() outside\n",
        "        #state, reward, done, _ = env.step(action) # HSG 17/3/2024\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        cumulative_returns += reward\n",
        "\n",
        "        if if_render:\n",
        "            env.render()\n",
        "        if done:\n",
        "            break\n",
        "    return cumulative_returns, episode_steps + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tzAw9k26nAC"
      },
      "source": [
        "##DRL Agent Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwCbbocm6PHM"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "# from elegantrl.agents import AgentA2C\n",
        "\n",
        "MODELS = {\"ppo\": AgentPPO}\n",
        "OFF_POLICY_MODELS = [\"ddpg\", \"td3\", \"sac\"]\n",
        "ON_POLICY_MODELS = [\"ppo\"]\n",
        "# MODEL_KWARGS = {x: config.__dict__[f\"{x.upper()}_PARAMS\"] for x in MODELS.keys()}\n",
        "#\n",
        "# NOISE = {\n",
        "#     \"normal\": NormalActionNoise,\n",
        "#     \"ornstein_uhlenbeck\": OrnsteinUhlenbeckActionNoise,\n",
        "# }\n",
        "\n",
        "\n",
        "class DRLAgent:\n",
        "    \"\"\"Implementations of DRL algorithms\n",
        "    Attributes\n",
        "    ----------\n",
        "        env: gym environment class\n",
        "            user-defined class\n",
        "    Methods\n",
        "    -------\n",
        "        get_model()\n",
        "            setup DRL algorithms\n",
        "        train_model()\n",
        "            train DRL algorithms in a train dataset\n",
        "            and output the trained model\n",
        "        DRL_prediction()\n",
        "            make a prediction in a test dataset and get results\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, price_array, tech_array, turbulence_array):\n",
        "        self.env = env\n",
        "        self.price_array = price_array\n",
        "        self.tech_array = tech_array\n",
        "        self.turbulence_array = turbulence_array\n",
        "\n",
        "    def get_model(self, model_name, model_kwargs):\n",
        "        env_config = {\n",
        "            \"price_array\": self.price_array,\n",
        "            \"tech_array\": self.tech_array,\n",
        "            \"turbulence_array\": self.turbulence_array,\n",
        "            \"if_train\": True,\n",
        "        }\n",
        "        environment = self.env(config=env_config)\n",
        "        env_args = {'config': env_config,\n",
        "              'env_name': environment.env_name,\n",
        "              'state_dim': environment.state_dim,\n",
        "              'action_dim': environment.action_dim,\n",
        "              'if_discrete': False}\n",
        "        agent = MODELS[model_name]\n",
        "        if model_name not in MODELS:\n",
        "            raise NotImplementedError(\"NotImplementedError\")\n",
        "        model = Config(agent_class=agent, env_class=self.env, env_args=env_args)\n",
        "        model.if_off_policy = model_name in OFF_POLICY_MODELS\n",
        "        if model_kwargs is not None:\n",
        "            try:\n",
        "                model.learning_rate = model_kwargs[\"learning_rate\"]\n",
        "                model.batch_size = model_kwargs[\"batch_size\"]\n",
        "                model.gamma = model_kwargs[\"gamma\"]\n",
        "                model.seed = model_kwargs[\"seed\"]\n",
        "                model.net_dims = model_kwargs[\"net_dimension\"]\n",
        "                model.target_step = model_kwargs[\"target_step\"]\n",
        "                model.eval_gap = model_kwargs[\"eval_gap\"]\n",
        "                model.eval_times = model_kwargs[\"eval_times\"]\n",
        "            except BaseException:\n",
        "                raise ValueError(\n",
        "                    \"Fail to read arguments, please check 'model_kwargs' input.\"\n",
        "                )\n",
        "        return model\n",
        "\n",
        "    def train_model(self, model, cwd, total_timesteps=5000):\n",
        "        model.cwd = cwd\n",
        "        model.break_step = total_timesteps\n",
        "        train_agent(model)\n",
        "\n",
        "    @staticmethod\n",
        "    def DRL_prediction(model_name, cwd, net_dimension, environment):\n",
        "        if model_name not in MODELS:\n",
        "            raise NotImplementedError(\"NotImplementedError\")\n",
        "        agent_class = MODELS[model_name]\n",
        "        environment.env_num = 1\n",
        "        agent = agent_class(net_dimension, environment.state_dim, environment.action_dim)\n",
        "        actor = agent.act\n",
        "        # load agent\n",
        "        try:\n",
        "            cwd = cwd + '/actor.pth'\n",
        "            print(f\"| load actor from: {cwd}\")\n",
        "            actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
        "            act = actor\n",
        "            device = agent.device\n",
        "        except BaseException:\n",
        "            raise ValueError(\"Fail to load agent!\")\n",
        "\n",
        "        # test on the testing env\n",
        "        _torch = torch\n",
        "        state = environment.reset()\n",
        "        episode_returns = []  # the cumulative_return / initial_account\n",
        "        episode_total_assets = [environment.initial_total_asset]\n",
        "        with _torch.no_grad():\n",
        "            for i in range(environment.max_step):\n",
        "                s_tensor = _torch.as_tensor((state,), device=device)\n",
        "                a_tensor = act(s_tensor)  # action_tanh = act.forward()\n",
        "                action = (\n",
        "                    a_tensor.detach().cpu().numpy()[0]\n",
        "                )  # not need detach(), because with torch.no_grad() outside\n",
        "                #state, reward, done, _ = environment.step(action) # HSG 17/3/2024\n",
        "                state, reward, done, _, _ = environment.step(action)\n",
        "\n",
        "                total_asset = (\n",
        "                    environment.amount\n",
        "                    + (\n",
        "                        environment.price_ary[environment.day] * environment.stocks\n",
        "                    ).sum()\n",
        "                )\n",
        "                episode_total_assets.append(total_asset)\n",
        "                episode_return = total_asset / environment.initial_total_asset\n",
        "                episode_returns.append(episode_return)\n",
        "                if done:\n",
        "                    break\n",
        "        print(\"Test Finished!\")\n",
        "        # return episode total_assets on testing data\n",
        "        print(\"episode_return\", episode_return)\n",
        "        return episode_total_assets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjLda8No6pvI"
      },
      "source": [
        "## Train & Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8-e03ev32oz"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from finrl.config import ERL_PARAMS\n",
        "from finrl.config import INDICATORS\n",
        "from finrl.config import RLlib_PARAMS\n",
        "from finrl.config import SAC_PARAMS\n",
        "from finrl.config import TRAIN_END_DATE\n",
        "from finrl.config import TRAIN_START_DATE\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "from finrl.meta.data_processor import DataProcessor\n",
        "\n",
        "# construct environment\n",
        "\n",
        "\n",
        "def train(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    ticker_list,\n",
        "    data_source,\n",
        "    time_interval,\n",
        "    technical_indicator_list,\n",
        "    drl_lib,\n",
        "    env,\n",
        "    model_name,\n",
        "    if_vix=True,\n",
        "    **kwargs,\n",
        "):\n",
        "    # download data\n",
        "    dp = DataProcessor(data_source, **kwargs)\n",
        "    data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "    data = dp.clean_data(data)\n",
        "    data = dp.add_technical_indicator(data, technical_indicator_list)\n",
        "    if if_vix:\n",
        "        data = dp.add_vix(data)\n",
        "    else:\n",
        "        data = dp.add_turbulence(data)\n",
        "    price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n",
        "    env_config = {\n",
        "        \"price_array\": price_array,\n",
        "        \"tech_array\": tech_array,\n",
        "        \"turbulence_array\": turbulence_array,\n",
        "        \"if_train\": True,\n",
        "    }\n",
        "    env_instance = env(config=env_config)\n",
        "\n",
        "    # read parameters\n",
        "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
        "\n",
        "    if drl_lib == \"elegantrl\":\n",
        "        DRLAgent_erl = DRLAgent\n",
        "        break_step = kwargs.get(\"break_step\", 1e6)\n",
        "        erl_params = kwargs.get(\"erl_params\")\n",
        "        agent = DRLAgent_erl(\n",
        "            env=env,\n",
        "            price_array=price_array,\n",
        "            tech_array=tech_array,\n",
        "            turbulence_array=turbulence_array,\n",
        "        )\n",
        "        model = agent.get_model(model_name, model_kwargs=erl_params)\n",
        "        trained_model = agent.train_model(\n",
        "            model=model, cwd=cwd, total_timesteps=break_step\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Evsg8QtEDHDO"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from finrl.config import INDICATORS\n",
        "from finrl.config import RLlib_PARAMS\n",
        "from finrl.config import TEST_END_DATE\n",
        "from finrl.config import TEST_START_DATE\n",
        "from finrl.config_tickers import DOW_30_TICKER\n",
        "\n",
        "def test(\n",
        "    start_date,\n",
        "    end_date,\n",
        "    ticker_list,\n",
        "    data_source,\n",
        "    time_interval,\n",
        "    technical_indicator_list,\n",
        "    drl_lib,\n",
        "    env,\n",
        "    model_name,\n",
        "    if_vix=True,\n",
        "    **kwargs,\n",
        "):\n",
        "\n",
        "    # import data processor\n",
        "    from finrl.meta.data_processor import DataProcessor\n",
        "\n",
        "    # fetch data\n",
        "    dp = DataProcessor(data_source, **kwargs)\n",
        "    data = dp.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "    data = dp.clean_data(data)\n",
        "    data = dp.add_technical_indicator(data, technical_indicator_list)\n",
        "\n",
        "    if if_vix:\n",
        "        data = dp.add_vix(data)\n",
        "    else:\n",
        "        data = dp.add_turbulence(data)\n",
        "    price_array, tech_array, turbulence_array = dp.df_to_array(data, if_vix)\n",
        "\n",
        "    env_config = {\n",
        "        \"price_array\": price_array,\n",
        "        \"tech_array\": tech_array,\n",
        "        \"turbulence_array\": turbulence_array,\n",
        "        \"if_train\": False,\n",
        "    }\n",
        "    env_instance = env(config=env_config)\n",
        "\n",
        "    # load elegantrl needs state dim, action dim and net dim\n",
        "    net_dimension = kwargs.get(\"net_dimension\", 2**7)\n",
        "    cwd = kwargs.get(\"cwd\", \"./\" + str(model_name))\n",
        "    print(\"price_array: \", len(price_array))\n",
        "\n",
        "    if drl_lib == \"elegantrl\":\n",
        "        DRLAgent_erl = DRLAgent\n",
        "        episode_total_assets = DRLAgent_erl.DRL_prediction(\n",
        "            model_name=model_name,\n",
        "            cwd=cwd,\n",
        "            net_dimension=net_dimension,\n",
        "            environment=env_instance,\n",
        "        )\n",
        "        return episode_total_assets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf5aVHAU-xF6"
      },
      "source": [
        "## Import Dow Jones 30 Symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx25TA_X87F-"
      },
      "outputs": [],
      "source": [
        "ticker_list = DOW_30_TICKER\n",
        "action_dim = len(DOW_30_TICKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIV0kO_y-inG"
      },
      "outputs": [],
      "source": [
        "print(ticker_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnqQ-cC5-rfO"
      },
      "outputs": [],
      "source": [
        "print(INDICATORS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZMkcyjZ-25l"
      },
      "source": [
        "## Calculate the DRL state dimension manually for paper trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLfkTsXK-e90"
      },
      "outputs": [],
      "source": [
        "# amount + (turbulence, turbulence_bool) + (price, shares, cd (holding time)) * stock_dim + tech_dim\n",
        "state_dim = 1 + 2 + 3 * action_dim + len(INDICATORS) * action_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqUkvImG-n66"
      },
      "outputs": [],
      "source": [
        "print(state_dim)\n",
        "print(action_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rwy7V72-8YY"
      },
      "source": [
        "## Get the API Keys Ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z6qlLXY-fA2"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"PKJNFLDXQ317SYY5482O\"\n",
        "API_SECRET = \"4JzjlzIAtRoezhp6epRLolXUfieCbgdgJ2GCaBFr\"\n",
        "API_BASE_URL = 'https://paper-api.alpaca.markets'\n",
        "data_url = 'wss://data.alpaca.markets'\n",
        "env = StockTradingEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J25MuZLiGqCP"
      },
      "source": [
        "## Show the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJZWm8NHtSN"
      },
      "source": [
        "### Step 1. Pick a data source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZCru8f7GqgL"
      },
      "outputs": [],
      "source": [
        "DP = DataProcessor(data_source = 'alpaca',\n",
        "                  API_KEY = API_KEY,\n",
        "                  API_SECRET = API_SECRET,\n",
        "                  API_BASE_URL = API_BASE_URL\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvPEW2mYHvkR"
      },
      "source": [
        "### Step 2. Get ticker list, Set start date and end date, specify the data frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPNxj6c8HIiE"
      },
      "outputs": [],
      "source": [
        "data = DP.download_data(start_date = '2021-10-04',\n",
        "                        end_date = '2021-10-08',\n",
        "                        ticker_list = ticker_list,\n",
        "                        time_interval= '1Min')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPcazCq1d5ec"
      },
      "outputs": [],
      "source": [
        "print(data['timestamp'].nunique())\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i46jGdE0IAel"
      },
      "source": [
        "### Step 3. Data Cleaning & Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9euUsEPHWFK"
      },
      "outputs": [],
      "source": [
        "data = DP.clean_data(data)\n",
        "data = DP.add_technical_indicator(data, INDICATORS)\n",
        "data = DP.add_vix(data)\n",
        "print(data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOcPTaAgHdxa"
      },
      "outputs": [],
      "source": [
        "data.shape\n",
        "#print(data[:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbu03L_UIMWt"
      },
      "source": [
        "### Step 4. Transform to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzj0vjZZHdGM"
      },
      "outputs": [],
      "source": [
        "price_array, tech_array, turbulence_array = DP.df_to_array(data, if_vix=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "778F_9aVJPNq"
      },
      "outputs": [],
      "source": [
        "#price_array\n",
        "print(tech_array)\n",
        "print(turbulence_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW0UDAXI1nEa"
      },
      "source": [
        "# Part 2: Train the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lArLOFcJ7VMO"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1F84mebj4gu"
      },
      "outputs": [],
      "source": [
        "ERL_PARAMS = {\"learning_rate\": 3e-6,\"batch_size\": 2048,\"gamma\":  0.985,\n",
        "        \"seed\":312,\"net_dimension\":[128,64], \"target_step\":5000, \"eval_gap\":30,\n",
        "        \"eval_times\":1}\n",
        "env = StockTradingEnv\n",
        "#if you want to use larger datasets (change to longer period), and it raises error,\n",
        "#please try to increase \"target_step\". It should be larger than the episode steps."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EO_fLUHoLMYO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxcNI2fdNjip"
      },
      "outputs": [],
      "source": [
        "train(start_date = '2022-08-25',\n",
        "      end_date = '2022-08-31',\n",
        "#train(start_date = '2024-01-01',\n",
        "#      end_date = '2024-03-17',\n",
        "      ticker_list = ticker_list,\n",
        "      data_source = 'alpaca',\n",
        "      time_interval= '1Min',\n",
        "      technical_indicator_list= INDICATORS,\n",
        "      drl_lib='elegantrl',\n",
        "      env=env,\n",
        "      model_name='ppo',\n",
        "      if_vix=True,\n",
        "      API_KEY = API_KEY,\n",
        "      API_SECRET = API_SECRET,\n",
        "      API_BASE_URL = API_BASE_URL,\n",
        "      erl_params=ERL_PARAMS,\n",
        "      cwd='./papertrading_erl', #current_working_dir\n",
        "      break_step=1e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g37WugV_1pAS"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxYoWCDa02TW"
      },
      "outputs": [],
      "source": [
        "account_value_erl=test(start_date = '2022-09-01',\n",
        "                      end_date = '2022-09-02',\n",
        "                      ticker_list = ticker_list,\n",
        "                      data_source = 'alpaca',\n",
        "                      time_interval= '1Min',\n",
        "                      technical_indicator_list= INDICATORS,\n",
        "                      drl_lib='elegantrl',\n",
        "                      env=env,\n",
        "                      model_name='ppo',\n",
        "                      if_vix=True,\n",
        "                      API_KEY = API_KEY,\n",
        "                      API_SECRET = API_SECRET,\n",
        "                      API_BASE_URL = API_BASE_URL,\n",
        "                      cwd='./papertrading_erl',\n",
        "                      net_dimension = ERL_PARAMS['net_dimension'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8aNQ58X7avM"
      },
      "source": [
        "## Use full data to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CQ9_Yv41r88"
      },
      "source": [
        "After tuning well, retrain on the training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUSgbwt_10V3"
      },
      "outputs": [],
      "source": [
        "train(start_date = '2022-08-25',\n",
        "      end_date = '2022-09-02',\n",
        "      ticker_list = ticker_list,\n",
        "      data_source = 'alpaca',\n",
        "      time_interval= '1Min',\n",
        "      technical_indicator_list= INDICATORS,\n",
        "      drl_lib='elegantrl',\n",
        "      env=env,\n",
        "      model_name='ppo',\n",
        "      if_vix=True,\n",
        "      API_KEY = API_KEY,\n",
        "      API_SECRET = API_SECRET,\n",
        "      API_BASE_URL = API_BASE_URL,\n",
        "      erl_params=ERL_PARAMS,\n",
        "      cwd='./papertrading_erl_retrain',\n",
        "      break_step=2e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIQN6Ggt7gXY"
      },
      "source": [
        "# Part 3: Deploy the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFoxkigg1zXa"
      },
      "source": [
        "## Setup Alpaca Paper trading environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hT2EQ1djoEi"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import threading\n",
        "from finrl.meta.data_processors.processor_alpaca import AlpacaProcessor\n",
        "import alpaca_trade_api as tradeapi\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "\n",
        "class AlpacaPaperTrading():\n",
        "\n",
        "    def __init__(self,ticker_list, time_interval, drl_lib, agent, cwd, net_dim,\n",
        "                 state_dim, action_dim, API_KEY, API_SECRET,\n",
        "                 API_BASE_URL, tech_indicator_list, turbulence_thresh=30,\n",
        "                 max_stock=1e2, latency = None):\n",
        "        #load agent\n",
        "        self.drl_lib = drl_lib\n",
        "        if agent =='ppo':\n",
        "            if drl_lib == 'elegantrl':\n",
        "                agent_class = AgentPPO\n",
        "                agent = agent_class(net_dim, state_dim, action_dim)\n",
        "                actor = agent.act\n",
        "                # load agent\n",
        "                try:\n",
        "                    cwd = cwd + '/actor.pth'\n",
        "                    print(f\"| load actor from: {cwd}\")\n",
        "                    actor.load_state_dict(torch.load(cwd, map_location=lambda storage, loc: storage))\n",
        "                    self.act = actor\n",
        "                    self.device = agent.device\n",
        "                except BaseException:\n",
        "                    raise ValueError(\"Fail to load agent!\")\n",
        "\n",
        "            elif drl_lib == 'rllib':\n",
        "                from ray.rllib.agents import ppo\n",
        "                from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
        "\n",
        "                config = ppo.DEFAULT_CONFIG.copy()\n",
        "                config['env'] = StockEnvEmpty\n",
        "                config[\"log_level\"] = \"WARN\"\n",
        "                config['env_config'] = {'state_dim':state_dim,\n",
        "                            'action_dim':action_dim,}\n",
        "                trainer = PPOTrainer(env=StockEnvEmpty, config=config)\n",
        "                trainer.restore(cwd)\n",
        "                try:\n",
        "                    trainer.restore(cwd)\n",
        "                    self.agent = trainer\n",
        "                    print(\"Restoring from checkpoint path\", cwd)\n",
        "                except:\n",
        "                    raise ValueError('Fail to load agent!')\n",
        "\n",
        "            elif drl_lib == 'stable_baselines3':\n",
        "                from stable_baselines3 import PPO\n",
        "\n",
        "                try:\n",
        "                    #load agent\n",
        "                    self.model = PPO.load(cwd)\n",
        "                    print(\"Successfully load model\", cwd)\n",
        "                except:\n",
        "                    raise ValueError('Fail to load agent!')\n",
        "\n",
        "            else:\n",
        "                raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Agent input is NOT supported yet.')\n",
        "\n",
        "\n",
        "\n",
        "        #connect to Alpaca trading API\n",
        "        try:\n",
        "            self.alpaca = tradeapi.REST(API_KEY,API_SECRET,API_BASE_URL, 'v2')\n",
        "        except:\n",
        "            raise ValueError('Fail to connect Alpaca. Please check account info and internet connection.')\n",
        "\n",
        "        #read trading time interval\n",
        "        if time_interval == '1s':\n",
        "            self.time_interval = 1\n",
        "        elif time_interval == '5s':\n",
        "            self.time_interval = 5\n",
        "        elif time_interval == '1Min':\n",
        "            self.time_interval = 60\n",
        "        elif time_interval == '5Min':\n",
        "            self.time_interval = 60 * 5\n",
        "        elif time_interval == '15Min':\n",
        "            self.time_interval = 60 * 15\n",
        "        else:\n",
        "            raise ValueError('Time interval input is NOT supported yet.')\n",
        "\n",
        "        #read trading settings\n",
        "        self.tech_indicator_list = tech_indicator_list\n",
        "        self.turbulence_thresh = turbulence_thresh\n",
        "        self.max_stock = max_stock\n",
        "\n",
        "        #initialize account\n",
        "        self.stocks = np.asarray([0] * len(ticker_list)) #stocks holding\n",
        "        self.stocks_cd = np.zeros_like(self.stocks)\n",
        "        self.cash = None #cash record\n",
        "        self.stocks_df = pd.DataFrame(self.stocks, columns=['stocks'], index = ticker_list)\n",
        "        self.asset_list = []\n",
        "        self.price = np.asarray([0] * len(ticker_list))\n",
        "        self.stockUniverse = ticker_list\n",
        "        self.turbulence_bool = 0\n",
        "        self.equities = []\n",
        "\n",
        "    def test_latency(self, test_times = 10):\n",
        "        total_time = 0\n",
        "        for i in range(0, test_times):\n",
        "            time0 = time.time()\n",
        "            self.get_state()\n",
        "            time1 = time.time()\n",
        "            temp_time = time1 - time0\n",
        "            total_time += temp_time\n",
        "        latency = total_time/test_times\n",
        "        print('latency for data processing: ', latency)\n",
        "        return latency\n",
        "\n",
        "    def run(self):\n",
        "        orders = self.alpaca.list_orders(status=\"open\")\n",
        "        for order in orders:\n",
        "          self.alpaca.cancel_order(order.id)\n",
        "\n",
        "        # Wait for market to open.\n",
        "        print(\"Waiting for market to open...\")\n",
        "        tAMO = threading.Thread(target=self.awaitMarketOpen)\n",
        "        tAMO.start()\n",
        "        tAMO.join()\n",
        "        print(\"Market opened.\")\n",
        "        while True:\n",
        "\n",
        "          # Figure out when the market will close so we can prepare to sell beforehand.\n",
        "          clock = self.alpaca.get_clock()\n",
        "          closingTime = clock.next_close.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
        "          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
        "          self.timeToClose = closingTime - currTime\n",
        "\n",
        "          #if(self.timeToClose < (60)):\n",
        "          if(self.timeToClose < (21600)):   ## HSG\n",
        "            # Close all positions when 1 minutes til market close.\n",
        "            print(\"Market closing soon. Stop trading.\")\n",
        "            break\n",
        "\n",
        "            # Close all positions when 1 minutes til market close.\n",
        "            print(\"Market closing soon.  Closing positions.\")\n",
        "\n",
        "            positions = self.alpaca.list_positions()\n",
        "            for position in positions:\n",
        "              if(position.side == 'long'):\n",
        "                orderSide = 'sell'\n",
        "              else:\n",
        "                orderSide = 'buy'\n",
        "              qty = abs(int(float(position.qty)))\n",
        "              respSO = []\n",
        "              tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n",
        "              tSubmitOrder.start()\n",
        "              tSubmitOrder.join()\n",
        "\n",
        "            # Run script again after market close for next trading day.\n",
        "            print(\"Sleeping until market close (15 minutes).\")\n",
        "            #time.sleep(60 * 15)\n",
        "            time.sleep(60)\n",
        "\n",
        "          else:\n",
        "            trade = threading.Thread(target=self.trade)\n",
        "            trade.start()\n",
        "            trade.join()\n",
        "            last_equity = float(self.alpaca.get_account().last_equity)\n",
        "            cur_time = time.time()\n",
        "            self.equities.append([cur_time,last_equity])\n",
        "            time.sleep(self.time_interval)\n",
        "\n",
        "    def awaitMarketOpen(self):\n",
        "        isOpen = self.alpaca.get_clock().is_open\n",
        "        while(not isOpen):\n",
        "          clock = self.alpaca.get_clock()\n",
        "          openingTime = clock.next_open.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
        "          currTime = clock.timestamp.replace(tzinfo=datetime.timezone.utc).timestamp()\n",
        "          timeToOpen = int((openingTime - currTime) / 60)\n",
        "          print(str(timeToOpen) + \" minutes til market open.\")\n",
        "          time.sleep(60)\n",
        "          isOpen = self.alpaca.get_clock().is_open\n",
        "\n",
        "    def trade(self):\n",
        "        state = self.get_state()\n",
        "\n",
        "        if self.drl_lib == 'elegantrl':\n",
        "            with torch.no_grad():\n",
        "                s_tensor = torch.as_tensor((state,), device=self.device)\n",
        "                a_tensor = self.act(s_tensor)\n",
        "                action = a_tensor.detach().cpu().numpy()[0]\n",
        "            action = (action * self.max_stock).astype(int)\n",
        "\n",
        "        elif self.drl_lib == 'rllib':\n",
        "            action = self.agent.compute_single_action(state)\n",
        "\n",
        "        elif self.drl_lib == 'stable_baselines3':\n",
        "            action = self.model.predict(state)[0]\n",
        "\n",
        "        else:\n",
        "            raise ValueError('The DRL library input is NOT supported yet. Please check your input.')\n",
        "\n",
        "        self.stocks_cd += 1\n",
        "        if self.turbulence_bool == 0:\n",
        "            min_action = 10  # stock_cd\n",
        "            for index in np.where(action < -min_action)[0]:  # sell_index:\n",
        "                sell_num_shares = min(self.stocks[index], -action[index])\n",
        "                qty =  abs(int(sell_num_shares))\n",
        "                respSO = []\n",
        "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'sell', respSO))\n",
        "                tSubmitOrder.start()\n",
        "                tSubmitOrder.join()\n",
        "                self.cash = float(self.alpaca.get_account().cash)\n",
        "                self.stocks_cd[index] = 0\n",
        "\n",
        "            for index in np.where(action > min_action)[0]:  # buy_index:\n",
        "                if self.cash < 0:\n",
        "                    tmp_cash = 0\n",
        "                else:\n",
        "                    tmp_cash = self.cash\n",
        "                buy_num_shares = min(tmp_cash // self.price[index], abs(int(action[index])))\n",
        "                if (buy_num_shares != buy_num_shares): # if buy_num_change = nan\n",
        "                    qty = 0 # set to 0 quantity\n",
        "                else:\n",
        "                    qty = abs(int(buy_num_shares))\n",
        "                qty = abs(int(buy_num_shares))\n",
        "                respSO = []\n",
        "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, self.stockUniverse[index], 'buy', respSO))\n",
        "                tSubmitOrder.start()\n",
        "                tSubmitOrder.join()\n",
        "                self.cash = float(self.alpaca.get_account().cash)\n",
        "                self.stocks_cd[index] = 0\n",
        "\n",
        "        else:  # sell all when turbulence\n",
        "            positions = self.alpaca.list_positions()\n",
        "            for position in positions:\n",
        "                if(position.side == 'long'):\n",
        "                    orderSide = 'sell'\n",
        "                else:\n",
        "                    orderSide = 'buy'\n",
        "                qty = abs(int(float(position.qty)))\n",
        "                respSO = []\n",
        "                tSubmitOrder = threading.Thread(target=self.submitOrder(qty, position.symbol, orderSide, respSO))\n",
        "                tSubmitOrder.start()\n",
        "                tSubmitOrder.join()\n",
        "\n",
        "            self.stocks_cd[:] = 0\n",
        "\n",
        "\n",
        "    def get_state(self):\n",
        "        alpaca = AlpacaProcessor(api=self.alpaca)\n",
        "        price, tech, turbulence = alpaca.fetch_latest_data(ticker_list = self.stockUniverse, time_interval='1Min',\n",
        "                                                     tech_indicator_list=self.tech_indicator_list)\n",
        "        turbulence_bool = 1 if turbulence >= self.turbulence_thresh else 0\n",
        "\n",
        "        turbulence = (self.sigmoid_sign(turbulence, self.turbulence_thresh) * 2 ** -5).astype(np.float32)\n",
        "\n",
        "        tech = tech * 2 ** -7\n",
        "        positions = self.alpaca.list_positions()\n",
        "        stocks = [0] * len(self.stockUniverse)\n",
        "        for position in positions:\n",
        "            ind = self.stockUniverse.index(position.symbol)\n",
        "            stocks[ind] = ( abs(int(float(position.qty))))\n",
        "\n",
        "        stocks = np.asarray(stocks, dtype = float)\n",
        "        cash = float(self.alpaca.get_account().cash)\n",
        "        self.cash = cash\n",
        "        self.stocks = stocks\n",
        "        self.turbulence_bool = turbulence_bool\n",
        "        self.price = price\n",
        "\n",
        "\n",
        "\n",
        "        amount = np.array(self.cash * (2 ** -12), dtype=np.float32)\n",
        "        scale = np.array(2 ** -6, dtype=np.float32)\n",
        "        state = np.hstack((amount,\n",
        "                    turbulence,\n",
        "                    self.turbulence_bool,\n",
        "                    price * scale,\n",
        "                    self.stocks * scale,\n",
        "                    self.stocks_cd,\n",
        "                    tech,\n",
        "                    )).astype(np.float32)\n",
        "        state[np.isnan(state)] = 0.0\n",
        "        state[np.isinf(state)] = 0.0\n",
        "        print(len(self.stockUniverse))\n",
        "        return state\n",
        "\n",
        "    def submitOrder(self, qty, stock, side, resp):\n",
        "        if(qty > 0):\n",
        "          try:\n",
        "            self.alpaca.submit_order(stock, qty, side, \"market\", \"day\")\n",
        "            print(\"Market order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | completed.\")\n",
        "            resp.append(True)\n",
        "          except:\n",
        "            print(\"Order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | did not go through.\")\n",
        "            resp.append(False)\n",
        "        else:\n",
        "          print(\"Quantity is 0, order of | \" + str(qty) + \" \" + stock + \" \" + side + \" | not completed.\")\n",
        "          resp.append(True)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_sign(ary, thresh):\n",
        "        def sigmoid(x):\n",
        "            return 1 / (1 + np.exp(-x * np.e)) - 0.5\n",
        "\n",
        "        return sigmoid(ary / thresh) * thresh\n",
        "\n",
        "class StockEnvEmpty(gym.Env):\n",
        "    #Empty Env used for loading rllib agent\n",
        "    def __init__(self,config):\n",
        "      state_dim = config['state_dim']\n",
        "      action_dim = config['action_dim']\n",
        "      self.env_num = 1\n",
        "      self.max_step = 10000\n",
        "      self.env_name = 'StockEnvEmpty'\n",
        "      self.state_dim = state_dim\n",
        "      self.action_dim = action_dim\n",
        "      self.if_discrete = False\n",
        "      self.target_return = 9999\n",
        "      self.observation_space = gym.spaces.Box(low=-3000, high=3000, shape=(state_dim,), dtype=np.float32)\n",
        "      self.action_space = gym.spaces.Box(low=-1, high=1, shape=(action_dim,), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        return\n",
        "\n",
        "    def step(self, actions):\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os4C4-4H7ns7"
      },
      "source": [
        "## Run Paper trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nw0i-0UN3-7"
      },
      "outputs": [],
      "source": [
        "print(DOW_30_TICKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsSBK9ION1t6"
      },
      "outputs": [],
      "source": [
        "state_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYtSv6P1N247"
      },
      "outputs": [],
      "source": [
        "action_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl9nulnAJtiI"
      },
      "outputs": [],
      "source": [
        "paper_trading_erl = AlpacaPaperTrading(ticker_list = DOW_30_TICKER,\n",
        "                                       time_interval = '1Min',\n",
        "                                       drl_lib = 'elegantrl',\n",
        "                                       agent = 'ppo',\n",
        "                                       cwd = './papertrading_erl_retrain',\n",
        "                                       net_dim = ERL_PARAMS['net_dimension'],\n",
        "                                       state_dim = state_dim,\n",
        "                                       action_dim= action_dim,\n",
        "                                       API_KEY = API_KEY,\n",
        "                                       API_SECRET = API_SECRET,\n",
        "                                       API_BASE_URL = API_BASE_URL,\n",
        "                                       tech_indicator_list = INDICATORS,\n",
        "                                       turbulence_thresh=30,\n",
        "                                       max_stock=1e2)\n",
        "paper_trading_erl.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzBZfYEUI1O"
      },
      "source": [
        "# Part 4: Check Portfolio Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chovN1UhTAht"
      },
      "outputs": [],
      "source": [
        "#install finrl library\n",
        "#from finrl.plot import backtest_stats\n",
        "!pip install alpaca_trade_api    # HSG 18/3/2024\n",
        "!pip install exchange_calendars  # HSG 18/3/2024\n",
        "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "!pip install git+https://github.com/etesys/FinRL.git\n",
        "import alpaca_trade_api as tradeapi\n",
        "import exchange_calendars as tc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytz\n",
        "import yfinance as yf\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime as dt\n",
        "from finrl.plot import backtest_stats\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaofxMNCfAR1"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pandas  ## HSG\n",
        "def get_trading_days(start, end):\n",
        "    nyse = tc.get_calendar('NYSE')\n",
        "    #df = nyse.sessions_in_range(pd.Timestamp(start,tz=pytz.UTC), ## HSG\n",
        "    #                            pd.Timestamp(end,tz=pytz.UTC))\n",
        "    df = nyse.sessions_in_range(pd.Timestamp(start,tz=None),\n",
        "                                pd.Timestamp(end,tz=None))\n",
        "    trading_days = []\n",
        "    for day in df:\n",
        "        trading_days.append(str(day)[:10])\n",
        "\n",
        "    return trading_days\n",
        "\n",
        "def alpaca_history(key, secret, url, start, end):\n",
        "    api = tradeapi.REST(key, secret, url, 'v2')\n",
        "    trading_days = get_trading_days(start, end)\n",
        "    df = pd.DataFrame()\n",
        "    for day in trading_days:\n",
        "        new_rows = api.get_portfolio_history(date_start = day,timeframe='5Min').df.iloc[:78]\n",
        "        #df = df.append(new_rows, ignore_index=True)\n",
        "        df = pd.concat([df, new_rows], ignore_index=True)\n",
        "        #df = df.append(api.get_portfolio_history(date_start = day,timeframe='5Min').df.iloc[:78], ignore_index=True)\n",
        "    equities = df.equity.values\n",
        "    cumu_returns = equities/equities[0]\n",
        "    cumu_returns = cumu_returns[~np.isnan(cumu_returns)]\n",
        "\n",
        "    return df, cumu_returns\n",
        "\n",
        "def DIA_history(start):\n",
        "    data_df = yf.download(['^DJI'],start=start, interval=\"5m\")\n",
        "    data_df = data_df.iloc[:]\n",
        "    baseline_returns = data_df['Adj Close'].values/data_df['Adj Close'].values[0]\n",
        "    return data_df, baseline_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHiZRVpURpx"
      },
      "source": [
        "## Get cumulative return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmmo-iG1s_rd"
      },
      "outputs": [],
      "source": [
        "#API_KEY = \"\"\n",
        "#API_SECRET = \"\"  # HSG 20/3/2024\n",
        "API_KEY = \"PKJNFLDXQ317SYY5482O\"\n",
        "API_SECRET = \"4JzjlzIAtRoezhp6epRLolXUfieCbgdgJ2GCaBFr\"\n",
        "API_BASE_URL = 'https://paper-api.alpaca.markets'\n",
        "data_url = 'wss://data.alpaca.markets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_YT7v-LSdfV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!pip install --upgrade pip\n",
        "!pip install ta-lib\n",
        "!pip install zipline-reloaded\n",
        "\n",
        "#import zipline\n",
        "#from zipline.utils.datetime_utils import tz_convert\n",
        "#import tz_convert\n",
        "start_date = datetime.datetime.strptime('2024-03-21', '%Y-%m-%d')\n",
        "end_date = datetime.datetime.strptime('2024-03-22', '%Y-%m-%d')\n",
        "utc_date_start = date_start.astimezone(datetime.timezone.utc)\n",
        "utc_date_end = date_end.astimezone(datetime.timezone.utc)\n",
        "utc_start_date = tz_convert(utc_date_start, from_tz, to_tz)\n",
        "utc_start_end = tz_convert(utc_date_end, from_tz, to_tz)\n",
        "\n",
        "\n",
        "start_date = datetime.datetime.strptime('2024-03-21', '%Y-%m-%d')\n",
        "end_date = datetime.datetime.strptime('2024-03-22', '%Y-%m-%d')\n",
        "\n",
        "# Localize the timestamps to UTC\n",
        "utc_start_date = start_date.tz_localize(\"UTC\")\n",
        "utc_end_date = end_date.tz_localize(\"UTC\")\n",
        "\n",
        "# Convert the timestamps to the desired time zone\n",
        "utc_start_date = utc_start_date.astimezone(to_tz)\n",
        "utc_end_date = utc_end_date.astimezone(to_tz)\n",
        "\n",
        "#utc_start_date = pd.to_datetime(date_start).tz_convert(\"US/Eastern\")\n",
        "#utc_start_end = pd.to_datetime(date_end).tz_convert(\"US/Eastern\")\n",
        "\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "start_date = datetime.strptime('2024-03-21', '%Y-%m-%d')\n",
        "end_date = datetime.strptime('2024-03-22', '%Y-%m-%d')\n",
        "\n",
        "utc_start_date = start_date.replace(tzinfo=pytz.UTC)\n",
        "utc_end_date = end_date.replace(tzinfo=pytz.UTC)\n",
        "\"\"\"\n",
        "\n",
        "df_erl, cumu_erl = alpaca_history(key=API_KEY,\n",
        "                                  secret=API_SECRET,\n",
        "                                  url=API_BASE_URL,\n",
        "                                  start='2024-03-21', #must be within 1 month\n",
        "                                  end='2024-03-22') #change the date if error occurs\n",
        "                                  #start='2022-09-01', #must be within 1 month       ## HSG\n",
        "                                  #end='2022-09-12') #change the date if error occurs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMcQjwHOS6Zb"
      },
      "outputs": [],
      "source": [
        "#df_djia, cumu_djia = DIA_history(start='2022-09-01')  ## HSG\n",
        "df_djia, cumu_djia = DIA_history(start='2024-03-21')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJXPwmx9Ts5o"
      },
      "outputs": [],
      "source": [
        "#df_erl.tail()\n",
        "df_erl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Iaw90FTNfU"
      },
      "outputs": [],
      "source": [
        "returns_erl = cumu_erl -1\n",
        "returns_dia = cumu_djia - 1\n",
        "returns_dia = returns_dia[:returns_erl.shape[0]]\n",
        "print('len of erl return: ', returns_erl.shape[0])\n",
        "print('len of dia return: ', returns_dia.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IawaMsDwZni"
      },
      "outputs": [],
      "source": [
        "print(returns_erl)\n",
        "print(returns_dia)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "returns_erl = [ 0.00000000e+00, -1.44773723e-03, -4.55166799e-04,  5.06431319e-04,\n",
        " -1.27316600e-03, -4.83905999e-04,  2.41428703e-03,  1.64774653e-03,\n",
        "  1.38453041e-03,  2.34185648e-03,  1.93319670e-03,  2.22388983e-03,\n",
        "  3.91853175e-03,  3.30714409e-03,  3.37433168e-03,  3.30898884e-03,\n",
        "  3.13131067e-03,  3.42501365e-03,  3.21490679e-03,  1.34375181e-03,\n",
        "  1.44162044e-03,  2.09310705e-03,  2.10747665e-03,  1.67405843e-03,\n",
        "  1.67561190e-03,  2.48244554e-03,  2.76381785e-03,  1.91338995e-03,\n",
        "  1.71842943e-03,  2.24680352e-03,  2.56176962e-03,  2.25505633e-03,\n",
        "  2.02019104e-03,  2.28884431e-03,  1.64318321e-03,  1.15481099e-03,\n",
        "  1.16403472e-03,  7.39063496e-04,  8.30815336e-04,  4.34971685e-04,\n",
        "  1.08063278e-03,  1.40064766e-03,  1.87125207e-03,  1.65133893e-03,\n",
        "  1.74076057e-03,  2.43341414e-03,  2.04436692e-03,  1.40210404e-03,\n",
        "  1.22850373e-03,  1.08471064e-03,  1.41297833e-03,  1.67570900e-03,\n",
        "  1.16374344e-03,  5.35850162e-04,  5.81968812e-04,  8.66156785e-04,\n",
        "  4.25553771e-04,  3.60113834e-04,  6.41971602e-04,  3.17296309e-04,\n",
        "  5.85269936e-04,  2.11854512e-04, -2.45836675e-04,  5.81871720e-04,\n",
        "  3.11470796e-04,  2.20204415e-04, -1.62823106e-04,  5.46530271e-04,\n",
        "  2.36030394e-04,  6.95177960e-05,  8.65282958e-04,  5.41384400e-04,\n",
        "  1.31074057e-03,  1.40258950e-03,  6.90808825e-04,  1.40540516e-03,\n",
        "  1.05529179e-03,  1.09704131e-03,  5.88959428e-04,  1.50181741e-03,\n",
        "  2.50254356e-03,  1.28627341e-03,  1.45317438e-03,  1.37142300e-03,\n",
        "  1.59191869e-03,  1.35268427e-03,  3.40112904e-04, -5.18470713e-04,\n",
        " -6.08766175e-04, -9.89463491e-04, -7.20907312e-04, -1.37210264e-03,\n",
        " -1.33889722e-03, -2.63721002e-03, -2.00562725e-03, -2.16388704e-03,\n",
        " -2.07631015e-03, -1.29540005e-03, -9.79948485e-04, -8.27514212e-04,\n",
        " -1.43628039e-03, -1.23296996e-03, -1.50269124e-03, -1.87833978e-03,\n",
        " -1.72153637e-03, -1.47220439e-03, -2.13019615e-03, -2.27165904e-03,\n",
        " -2.27544563e-03, -2.83634550e-03, -3.25345227e-03, -3.73192113e-03,\n",
        " -3.45831617e-03, -3.07140497e-03, -3.00878070e-03, -3.41588701e-03,\n",
        " -3.34471865e-03, -2.61060684e-03]\n",
        "returns_dia = [ 0.00000000e+00,  6.46343084e-04,  1.27959991e-03,  5.31321762e-06,\n",
        "  6.97507402e-04,  2.08937363e-03,  1.46123324e-03,  2.01587412e-03,\n",
        "  2.44939332e-03,  2.93201059e-03,  3.10400141e-03,  3.99140715e-03,\n",
        "  3.54430973e-03,  3.86999029e-03,  4.39717955e-03,  3.97103982e-03,\n",
        "  3.90068888e-03,  3.73902931e-03,  3.13174822e-03,  3.30196797e-03,\n",
        "  3.40144321e-03,  3.27628741e-03,  2.82545106e-03,  2.97855044e-03,\n",
        "  3.37103980e-03,  3.34555603e-03,  2.93299452e-03,  2.68789776e-03,\n",
        "  3.18035432e-03,  3.77327005e-03,  3.71757966e-03,  3.52689418e-03,\n",
        "  3.72545109e-03,  3.59488369e-03,  3.36267640e-03,  3.46372593e-03,\n",
        "  2.94057078e-03,  2.83224017e-03,  2.72469671e-03,  3.21429988e-03,\n",
        "  3.55769116e-03,  3.81508704e-03,  3.43971805e-03,  3.57776332e-03,\n",
        "  4.20373907e-03,  3.87402440e-03,  3.23574953e-03,  3.25080365e-03,\n",
        "  3.01662850e-03,  3.16618574e-03,  3.43046912e-03,  3.09317819e-03,\n",
        "  2.80498533e-03,  3.00423099e-03,  3.31987548e-03,  2.92335201e-03,\n",
        "  2.96920311e-03,  3.26448026e-03,  3.00983939e-03,  3.30875708e-03,\n",
        "  2.91429987e-03,  2.39517883e-03,  2.85359144e-03,  2.79317818e-03,\n",
        "  2.70501813e-03,  2.40305026e-03,  2.79091515e-03,  2.90682200e-03,\n",
        "  2.59668752e-03,  2.99842581e-03,  2.71505421e-03,  3.18665147e-03,\n",
        "  3.08137104e-03,  2.65769113e-03,  3.35155800e-03,  2.83420803e-03,\n",
        "  2.58005912e-03,  1.84230901e-03,  1.70121357e-03,  1.91462781e-03,\n",
        "  8.76779301e-04,  7.10790446e-04,  7.59691727e-04,  5.37323730e-04,\n",
        "  4.06854720e-04,  3.77632023e-04, -6.01967878e-04, -7.55854403e-04,\n",
        " -9.24893438e-04, -7.89111210e-04, -1.01177439e-03, -1.30774029e-03,\n",
        " -2.09872096e-03, -1.87300760e-03, -2.26894071e-03, -2.67353239e-03,\n",
        " -2.17782887e-03, -2.08406042e-03, -1.88009190e-03, -2.22161372e-03,\n",
        " -1.86493939e-03, -2.00475572e-03, -2.22436872e-03, -2.32591021e-03,\n",
        " -2.17222047e-03, -2.33525754e-03, -2.42941956e-03, -2.37884560e-03,\n",
        " -2.64726148e-03, -2.90121361e-03, -3.37851766e-03, -3.47199093e-03,\n",
        " -3.07900961e-03, -3.21252880e-03, -3.72919002e-03, -3.78507720e-03,\n",
        " -3.46795682e-03]"
      ],
      "metadata": {
        "id": "8W7-Rko-dvUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "type(returns_dia)\n",
        "#returns_erl = np.array(returns_erl)\n",
        "type(returns_erl)"
      ],
      "metadata": {
        "id": "hO9VDyfofSG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z0LEm7KUZ5W"
      },
      "source": [
        "## plot and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foqk1wIQTQJ3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "plt.figure(dpi=1000)\n",
        "plt.grid()\n",
        "plt.grid(which='minor', axis='y')\n",
        "plt.title('Stock Trading (Paper trading)', fontsize=20)\n",
        "plt.plot(returns_erl, label = 'ElegantRL Agent', color = 'red')\n",
        "#plt.plot(returns_sb3, label = 'Stable-Baselines3 Agent', color = 'blue' )\n",
        "#plt.plot(returns_rllib, label = 'RLlib Agent', color = 'green')\n",
        "plt.plot(returns_dia, label = 'DJIA', color = 'grey')\n",
        "plt.ylabel('Return', fontsize=16)\n",
        "plt.xlabel('Year 2024', fontsize=16)\n",
        "plt.xticks(size = 14)\n",
        "plt.yticks(size = 14)\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(78))\n",
        "ax.xaxis.set_minor_locator(ticker.MultipleLocator(6))\n",
        "ax.yaxis.set_minor_locator(ticker.MultipleLocator(0.005))\n",
        "ax.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=2))\n",
        "#ax.xaxis.set_major_locator(ticker.FixedLocator([0, 2, 4, 6])) ## HSG\n",
        "ax.xaxis.set_major_formatter(ticker.FixedFormatter(['','10-19','','10-20',\n",
        "                                                    '','10-21','','10-22']))\n",
        "plt.legend(fontsize=10.5)\n",
        "plt.savefig('papertrading_stock.png')\n",
        "plt.show() # HSG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "# Create some data\n",
        "data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "# Create a plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot the data\n",
        "ax.plot(data)\n",
        "\n",
        "# Set the major tick labels\n",
        "ax.xaxis.set_major_formatter(ticker.FixedFormatter(['','10-19','','10-20','','10-21','','10-22']))\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFT7Dlz5lRV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_LsHVj_TZGL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0EVJIQUR6_fu",
        "9tzAw9k26nAC",
        "zjLda8No6pvI",
        "pf5aVHAU-xF6",
        "rZMkcyjZ-25l",
        "3rwy7V72-8YY",
        "J25MuZLiGqCP",
        "eW0UDAXI1nEa",
        "UFoxkigg1zXa"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "005d14239094016f48a03a57365c4ccb734e3f38c20ed0ca595d84f773bc39cd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}